{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_applied_modeling_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliotgunn/DS-Unit-2-Applied-Modeling/blob/master/assignment_applied_modeling_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCc3XZEyG3XV",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science, Unit 2: Predictive Modeling\n",
        "\n",
        "# Applied Modeling, Module 1\n",
        "\n",
        "You will use your portfolio project dataset for all assignments this sprint.\n",
        "\n",
        "## Assignment\n",
        "\n",
        "Complete these tasks for your project, and document your decisions.\n",
        "\n",
        "- [x] Choose your target. Which column in your tabular dataset will you predict?\n",
        "- [x] Choose which observations you will use to train, validate, and test your model. And which observations, if any, to exclude.\n",
        "- [x] Determine whether your problem is regression or classification.\n",
        "- [x] Choose your evaluation metric.\n",
        "- [x] Begin with baselines: majority class baseline for classification, or mean baseline for regression, with your metric of choice.\n",
        "- [x] Begin to clean and explore your data.\n",
        "- [x] Choose which features, if any, to exclude. Would some features \"leak\" information from the future? **No**\n",
        "\n",
        "## Reading\n",
        "- [Attacking discrimination with smarter machine learning](https://research.google.com/bigpicture/attacking-discrimination-in-ml/), by Google Research, with  interactive visualizations. _\"A threshold classifier essentially makes a yes/no decision, putting things in one category or another. We look at how these classifiers work, ways they can potentially be unfair, and how you might turn an unfair classifier into a fairer one. As an illustrative example, we focus on loan granting scenarios where a bank may grant or deny a loan based on a single, automatically computed number such as a credit score.\"_\n",
        "- [How Shopify Capital Uses Quantile Regression To Help Merchants Succeed](https://engineering.shopify.com/blogs/engineering/how-shopify-uses-machine-learning-to-help-our-merchants-grow-their-business)\n",
        "- [Maximizing Scarce Maintenance Resources with Data: Applying predictive modeling, precision at k, and clustering to optimize impact](https://towardsdatascience.com/maximizing-scarce-maintenance-resources-with-data-8f3491133050), **by Lambda DS3 student** Michael Brady. His blog post extends the Tanzania Waterpumps scenario, far beyond what's in the lecture notebook.\n",
        "- [Notebook about how to calculate expected value from a confusion matrix by treating it as a cost-benefit matrix](https://github.com/podopie/DAT18NYC/blob/master/classes/13-expected_value_cost_benefit_analysis.ipynb)\n",
        "- [Simple guide to confusion matrix terminology](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) by Kevin Markham, with video\n",
        "- [Visualizing Machine Learning Thresholds to Make Better Business Decisions](https://blog.insightdatascience.com/visualizing-machine-learning-thresholds-to-make-better-business-decisions-4ab07f823415)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U07yNdJWuF9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you're in Colab...\n",
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if in_colab:\n",
        "    # Install required python package:\n",
        "    # category_encoders, version >= 2.0\n",
        "    !pip install --upgrade category_encoders\n",
        "    \n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Applied-Modeling.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjX_ppJ0frK",
        "colab_type": "text"
      },
      "source": [
        "## Research\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S0968090X18311021\n",
        "\n",
        "https://academic.oup.com/tse/advance-article/doi/10.1093/tse/tdy001/5306170"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wm16ZcBuxcd",
        "colab_type": "text"
      },
      "source": [
        "## Import multiple files from directory and `concat` into single df\n",
        "\n",
        "Example: \n",
        "\n",
        "https://stackoverflow.com/questions/20908018/import-multiple-excel-files-into-python-pandas-and-concatenate-them-into-one-dat\n",
        "https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROYUg9RTut8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is from 2014 - April 2017\n",
        "df_2014_to_2017_url = 'https://www.toronto.ca/ext/open_data/catalog/data_set_files/Subway%20&%20SRT%20Logs%20(Jan01_14%20to%20April30_17).xlsx'\n",
        "\n",
        "# TRAIN\n",
        "df_2017_May_url = \"https://www.toronto.ca/ext/open_data/catalog/data_set_files/Subway%20&%20SRT%20Logs%20(May%202017).xlsx\"\n",
        "\n",
        "df_2017_June_url = \"https://www.toronto.ca/ext/open_data/catalog/data_set_files/SubwayDelay201706.xlsx\"\n",
        "\n",
        "\n",
        "# TEST\n",
        "df_2019_Jan_url = \"https://www.toronto.ca/ext/open_data/catalog/data_set_files/Subway_&_SRT_Logs_January_2019.xlsx\"\n",
        "\n",
        "df_2019_Feb_url = \"https://www.toronto.ca/ext/open_data/catalog/data_set_files/Subway_&_SRT_Logs_February2019.xlsx\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_excel(df_2014_to_2017_url)\n",
        "df2 = pd.read_excel(df_2017_May_url)\n",
        "df3 = pd.read_excel(df_2017_June_url)\n",
        "\n",
        "test1 = pd.read_excel(df_2019_Jan_url)\n",
        "test2 = pd.read_excel(df_2019_Feb_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UelzmJbMv9x6",
        "colab_type": "code",
        "outputId": "bddac4d6-d293-4a61-f4cf-5774e43830a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(69016, 10)\n",
            "(1634, 10)\n",
            "(1568, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1EEMLwb39dQ",
        "colab_type": "text"
      },
      "source": [
        "Keep in mind that unlike the append() and extend() methods of Python lists, the append() method in Pandas does not modify the original object–instead it creates a new object with the combined data. It also is not a very efficient method, because it involves creation of a new index and data buffer. Thus, if you plan to do multiple append operations, it is generally better to build a list of DataFrames and pass them all at once to the concat() function. [link text](https://jakevdp.github.io/PythonDataScienceHandbook/03.06-concat-and-append.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs3IDOxI16Zl",
        "colab_type": "code",
        "outputId": "e4cc40e3-e613-4d78-a159-88f40856f1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# in future: try to do this programmatically...\n",
        "\n",
        "train_dfs = [df1, df2, df3]\n",
        "train = pd.concat(train_dfs)\n",
        "\n",
        "test_dfs = [test1, test2]\n",
        "test = pd.concat(test_dfs)\n",
        "\n",
        "all_dfs = train_dfs + test_dfs\n",
        "all = pd.concat(all_dfs)\n",
        "\n",
        "train.shape, test.shape, all.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((72218, 10), (3469, 10), (75687, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4LvYmYbEMps",
        "colab_type": "text"
      },
      "source": [
        "## Baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyxZjlaZEOo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# majority class baseline\n",
        "\n",
        "all['Code'].value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-FaRqb4FAgV",
        "colab_type": "code",
        "outputId": "07810a95-beff-4c67-d678-d85fe08ca33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# majority: MUSC     0.178512\n",
        "\n",
        "majority_class = all['Code'].mode()[0]\n",
        "pred = [majority_class] * len(all['Code'])\n",
        "\n",
        "# use metric: accuracy for classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(all['Code'], pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1785115013146247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JDYjAk72aD",
        "colab_type": "text"
      },
      "source": [
        "## Choose your target. Which column in your tabular dataset will you predict?\n",
        "\n",
        "1. Predict delay class:  `Code` (Classification)\n",
        "2. Predict delay time:  `Min Time` (Regression)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hef5b3Iu0wH-",
        "colab_type": "text"
      },
      "source": [
        "## Feature engineering approaches:\n",
        "\n",
        "> But Stockholmståg has found a way to use that data to also predict the ripple effect a single delay has on its entire system. An accident somewhere along its route means a train will be delayed before it rolls into the next station. But that also affects the train behind it, and the train behind it, and so forth. Eventually a single incident can throw off the scheduling of an entire commuter system, even if the original source of the disruption has already been resolved. [link](https://gizmodo.com/a-new-algorithm-can-predict-subway-delays-two-hours-bef-1729539784)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bNmEPsKFg3v",
        "colab_type": "text"
      },
      "source": [
        "## Wrangle + train/val/test + pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJDrYnQ1-Ipa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrangle(X):\n",
        "  \n",
        "  X = X.copy()\n",
        "  \n",
        "  # drop 'Code' values with less than 1 in a class\n",
        "  X = X[X.groupby('Code').Day.transform(len) >1]\n",
        "  \n",
        "  # Convert 'Date' to datetime\n",
        "  X['Date'] = pd.to_datetime(X['Date'], infer_datetime_format=True)\n",
        "  \n",
        "  # Extract components from 'Date', then drop original column\n",
        "  X['year'] = X['Date'].dt.year\n",
        "  X['month'] = X['Date'].dt.month\n",
        "  X['day'] = X['Date'].dt.day\n",
        "  X = X.drop(columns='Date')\n",
        "  \n",
        "  # 'Time' is a timestamp: we have hour and minute information\n",
        "  # Extract components from 'Time', then drop original column\n",
        "  X['Time'] = pd.to_datetime(X['Time'], infer_datetime_format=True)\n",
        "  X['hour'] = X['Time'].dt.hour\n",
        "  X['minute'] = X['Time'].dt.minute\n",
        "  X = X.drop(columns='Time')\n",
        "  \n",
        "  # Return wrangled dataframe\n",
        "  return X\n",
        "\n",
        "train = wrangle(train)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmQDM2rS5iS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split train into train & val. Make val the same size as test.\n",
        "train, val = train_test_split(train, test_size=len(test),  \n",
        "                              stratify=train['Code'], random_state=42)\n",
        "\n",
        "# arrange data into X features matrix and y target vector\n",
        "target = 'Code'\n",
        "X_train = train.drop(columns=target)\n",
        "y_train = train[target]\n",
        "X_val = val.drop(columns=target)\n",
        "y_val = val[target]\n",
        "X_test = test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU2_Y7-26x-g",
        "colab_type": "code",
        "outputId": "618e8de2-5652-4f03-c2ad-778af242cc21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='median'), \n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 0.4238122827346466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TljrF_ooJts5",
        "colab_type": "text"
      },
      "source": [
        "## Eval metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_eQmIxrIkai",
        "colab_type": "code",
        "outputId": "effc434b-fd3d-4d0f-fdc6-0361877548d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# generate predicted\n",
        "y_pred = pipeline.predict(X_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision Score (micro):  0.4238122827346466\n",
            "Precision Score (macro):  0.17160970923370134\n",
            "Precision Score (weighted):  0.3726795602253628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "124hXpE4J0QX",
        "colab_type": "code",
        "outputId": "1cac885b-f255-42fc-d775-98d09e3161a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# print('Precision,' precision_score(y_val, y_pred))\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "print('Precision Score (micro): ', precision_score(y_val, y_pred, average='micro'))\n",
        "print('Precision Score (macro): ', precision_score(y_val, y_pred, average='macro'))\n",
        "print('Precision Score (weighted): ', precision_score(y_val, y_pred, average='weighted'))\n",
        "print('')\n",
        "print('Recall Score (micro): ', recall_score(y_val, y_pred, average='micro'))\n",
        "print('Recall Score (macro): ', recall_score(y_val, y_pred, average='macro'))\n",
        "print('Recall Score (weighted): ', recall_score(y_val, y_pred, average='weighted'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision Score (micro):  0.4238122827346466\n",
            "Precision Score (macro):  0.17160970923370134\n",
            "Precision Score (weighted):  0.3726795602253628\n",
            "\n",
            "Recall Score (micro):  0.4238122827346466\n",
            "Recall Score (macro):  0.13043691600932608\n",
            "Recall Score (weighted):  0.4238122827346466\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX34fIuaIHOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}