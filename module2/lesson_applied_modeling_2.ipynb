{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson_applied_modeling_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3b4ePYmnR2Mp"
      },
      "source": [
        "Lambda School Data Science, Unit 2: Predictive Modeling\n",
        "\n",
        "# Applied Modeling, Module 2\n",
        "\n",
        "#### Objectives\n",
        "- log-transform regression target with right-skewed distribution\n",
        "- Understand why accuracy is a misleading metric when classes are imbalanced\n",
        "- Use classification metric: ROC AUC\n",
        "- Visualize the ROC curve by plotting true positive rate vs false positive rate at varying thresholds\n",
        "- Use the class_weight parameter in scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYCboUX-Wnf0",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "\n",
        "#### If you're using [Anaconda](https://www.anaconda.com/distribution/) locally\n",
        "\n",
        "Install required Python package, if you haven't already:\n",
        "\n",
        "[category_encoders](http://contrib.scikit-learn.org/categorical-encoding/), version >= 2.0  \n",
        "\n",
        "`conda install -c conda-forge category_encoders`\n",
        "\n",
        "or\n",
        "\n",
        "`pip install --upgrade category_encoders`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cr5P-ilWxa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you're in Colab...\n",
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if in_colab:\n",
        "    # Install required python package:\n",
        "    # category_encoders, version >= 2.0\n",
        "    !pip install --upgrade category_encoders\n",
        "    \n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Applied-Modeling.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3rqKsCPnYDm",
        "colab_type": "text"
      },
      "source": [
        "# Log-transform regression target with right-skewed distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO2jSHOxnRMV",
        "colab_type": "text"
      },
      "source": [
        "## Moore's Law dataset\n",
        "\n",
        "#### Background\n",
        "- https://en.wikipedia.org/wiki/Moore%27s_law\n",
        "- https://en.wikipedia.org/wiki/Transistor_count\n",
        "\n",
        "#### Scrape HTML tables with Pandas!\n",
        "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html\n",
        "- https://medium.com/@ageitgey/quick-tip-the-easiest-way-to-grab-data-out-of-a-web-page-in-python-7153cecfca58\n",
        "\n",
        "#### More web scraping options\n",
        "- https://automatetheboringstuff.com/chapter11/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28Cozbg4npYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scrape data\n",
        "import pandas as pd\n",
        "tables = pd.read_html('https://en.wikipedia.org/wiki/Transistor_count', header=0)\n",
        "moore = tables[0]\n",
        "moore = moore[['Date of introduction', 'Transistor count']].dropna()\n",
        "\n",
        "# Clean data\n",
        "for column in moore:\n",
        "    moore[column] = (moore[column]\n",
        "                     .str.split('[').str[0]  # Remove citations\n",
        "                     .str.replace(r'\\D','')  # Remove non-digit characters\n",
        "                     .astype(int))\n",
        "    \n",
        "moore = moore.sort_values(by='Date of introduction')\n",
        "\n",
        "# Plot relationship between date & transistors\n",
        "%matplotlib inline\n",
        "moore.plot(x='Date of introduction', y='Transistor count', \n",
        "           kind='scatter', alpha=0.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in6gDvsSqlz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "features = ['Date of introduction']\n",
        "target = 'Transistor count'\n",
        "X = moore[features]\n",
        "y = moore[target]\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot line of best fit\n",
        "ax = moore.plot(x='Date of introduction', y='Transistor count', kind='scatter', alpha=0.5)\n",
        "ax.plot(X, y_pred);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ChYm-19oMzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot distribution of transistor counts\n",
        "import seaborn as sns\n",
        "y = moore['Transistor count']\n",
        "sns.distplot(y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjtcSh_woVT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Describe distribution of transistor counts\n",
        "pd.options.display.float_format = '{:,.0f}'.format\n",
        "y.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vjC-0SmqXs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Log-transform the target\n",
        "import numpy as np\n",
        "y_log = np.log1p(y)\n",
        "moore['log(Transistor count)'] = y_log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFNjgPPpqZkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot distribution of log-transformed target\n",
        "sns.distplot(moore['log(Transistor count)']);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCSCG5TNrRzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Describe distribution of log-transformed target\n",
        "y_log.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTdA02f4raZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit Linear Regression with log-transformed target\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "features = ['Date of introduction']\n",
        "target = 'log(Transistor count)'\n",
        "X = moore[features]\n",
        "y_log = moore[target]\n",
        "model.fit(X, y_log)\n",
        "y_pred_log = model.predict(X)\n",
        "\n",
        "# Plot line of best fit, in units of log-transistors\n",
        "ax = moore.plot(x='Date of introduction', y='log(Transistor count)', kind='scatter', alpha=0.5)\n",
        "ax.plot(X, y_pred_log);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5F45r_frpDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert log-transistors to transistors\n",
        "y_pred = np.expm1(y_pred_log)\n",
        "\n",
        "# Plot line of best fit, in units of transistors\n",
        "ax = moore.plot(x='Date of introduction', y='Transistor count', kind='scatter', alpha=0.5)\n",
        "ax.plot(X, y_pred);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7BsKljnkaA",
        "colab_type": "text"
      },
      "source": [
        "#### Terence Parr & Jeremy Howard, [The Mechanics of Machine Learning, Chapter 5.5](https://mlbook.explained.ai/prep.html#logtarget)\n",
        "\n",
        "> Transforming the target variable (using the mathematical log function) into a tighter, more uniform space makes life easier for any model.\n",
        "\n",
        "> The only problem is that, while easy to execute, understanding why taking the log of the target variable works and how it affects the training/testing process is intellectually challenging. You can skip this section for now, if you like, but just remember that this technique exists and check back here if needed in the future.\n",
        "\n",
        "> Optimally, the distribution of prices would be a narrow ‚Äúbell curve‚Äù distribution without a tail. This would make predictions based upon average prices more accurate. We need a mathematical operation that transforms the widely-distributed target prices into a new space. The ‚Äúprice in dollars space‚Äù has a long right tail because of outliers and we want to squeeze that space into a new space that is normally distributed (‚Äúbell curved‚Äù). More specifically, we need to shrink large values a lot and smaller values a little. That magic operation is called the logarithm or log for short. \n",
        "\n",
        "> To make actual predictions, we have to take the exp of model predictions to get prices in dollars instead of log dollars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8erKO84n1DD",
        "colab_type": "text"
      },
      "source": [
        "#### Wikipedia, [Logarithm](https://en.wikipedia.org/wiki/Logarithm)\n",
        "\n",
        "> Addition, multiplication, and exponentiation are three fundamental arithmetic operations. Addition can be undone by subtraction. Multiplication can be undone by division. The idea and purpose of **logarithms** is also to **undo** a fundamental arithmetic operation, namely raising a number to a certain power, an operation also known as **exponentiation.** \n",
        "\n",
        "> For example, raising 2 to the third power yields 8.\n",
        "\n",
        "> The logarithm (with respect to base 2) of 8 is 3, reflecting the fact that 2 was raised to the third power to get 8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf0uQewaosuM",
        "colab_type": "text"
      },
      "source": [
        "### Use Numpy for exponents and logarithms functions\n",
        "- https://docs.scipy.org/doc/numpy/reference/routines.math.html#exponents-and-logarithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5hhIoL7v3jP",
        "colab_type": "text"
      },
      "source": [
        "# Shift to CLASSIFICATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mqj_gSCzR2Mq"
      },
      "source": [
        "## Lending Club üè¶\n",
        "\n",
        "This lecture uses Lending Club data, historical and current. Predict if peer-to-peer loans are charged off or fully paid. Decide which loans to invest in.\n",
        "\n",
        "#### Background\n",
        "\n",
        "[According to Wikipedia,](https://en.wikipedia.org/wiki/Lending_Club)\n",
        "\n",
        "> Lending Club is the world's largest peer-to-peer lending platform. Lending Club enables borrowers to create unsecured personal loans between \\\\$1,000 and \\\\$40,000. The standard loan period is three years. **Investors can search and browse the loan listings on Lending Club website and select loans that they want to invest in based on the information supplied about the borrower, amount of loan, loan grade, and loan purpose.** Investors make money from interest. Lending Club makes money by charging borrowers an origination fee and investors a service fee.\n",
        "\n",
        "Lending Club's article about [Benefits of diversification](https://www.lendingclub.com/investing/investor-education/benefits-of-diversification) explains,\n",
        "\n",
        "> **With the investment minimum of \\\\$1,000, you can get up to 40 Notes at \\$25 each.**\n",
        "\n",
        "You can read more good context in [Data-Driven Investment Strategies for Peer-to-Peer Lending: A Case Study for Teaching Data Science](https://www.liebertpub.com/doi/full/10.1089/big.2018.0092):\n",
        "\n",
        "> Current refers to a loan that is still being reimbursed in a timely manner. Late corresponds to a loan on which a payment is between 16 and 120 days overdue. If the payment is delayed by more than 121 days, the loan is considered to be in Default. If LendingClub has decided that the loan will not be paid off, then it is given the status of Charged-Off.\n",
        "\n",
        "> These dynamics imply that 5 months after the term of each loan has ended, every loan ends in one of two LendingClub states‚Äîfully paid or charged-off. We call these two statuses fully paid and defaulted, respectively, and we refer to a loan that has reached one of these statuses as expired. **One way to simplify the problem is to consider only loans that have expired at the time of analysis.**\n",
        "\n",
        "> A significant portion (13.5%) of loans ended in Default status; depending on how much of the loan was paid back, these loans\n",
        "might have resulted in a significant loss to investors who had invested in them. The remainder was Fully Paid‚Äîthe borrower fully reimbursed the loan‚Äôs outstanding balance with interest, and the investor earned a positive return on his or her investment. Therefore, to avoid unsuccessful investments, **our goal is to estimate which loans are more likely to default and which will yield low returns.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0X_ZtsxXR2Mt",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_columns = 200\n",
        "pd.options.display.max_rows = 200\n",
        "\n",
        "history_location = '../data/lending-club/lending-club-subset.csv'\n",
        "current_location = '../data/lending-club/primaryMarketNotes_browseNotes_1-RETAIL.csv'\n",
        "\n",
        "# Stratified sample, 10% of expired Lending Club loans, grades A-D\n",
        "# Source: https://www.lendingclub.com/info/download-data.action\n",
        "history = pd.read_csv(history_location)\n",
        "history['issue_d'] = pd.to_datetime(history['issue_d'], infer_datetime_format=True)\n",
        "\n",
        "# Current loans available for manual investing, June 17, 2019\n",
        "# Source: https://www.lendingclub.com/browse/browse.action\n",
        "current = pd.read_csv(current_location)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DqBdyqK5R2M2"
      },
      "source": [
        "### Wrangle data\n",
        "- Engineer date-based features\n",
        "- Remove features to avoid leakage\n",
        "- Do 3-way split, train/validate/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tQ9J0BH6R2M2",
        "colab": {}
      },
      "source": [
        "# Engineer date-based features\n",
        "\n",
        "# Transform earliest_cr_line to an integer:\n",
        "# How many days the earliest credit line was open, before the loan was issued.\n",
        "# For current loans available for manual investing, assume the loan will be issued today.\n",
        "history['earliest_cr_line'] = pd.to_datetime(history['earliest_cr_line'], infer_datetime_format=True)\n",
        "history['earliest_cr_line'] = history['issue_d'] - history['earliest_cr_line']\n",
        "history['earliest_cr_line'] = history['earliest_cr_line'].dt.days\n",
        "\n",
        "current['earliest_cr_line'] = pd.to_datetime(current['earliest_cr_line'], infer_datetime_format=True)\n",
        "current['earliest_cr_line'] = pd.Timestamp.today() - current['earliest_cr_line']\n",
        "current['earliest_cr_line'] = current['earliest_cr_line'].dt.days\n",
        "\n",
        "# Transform earliest_cr_line for the secondary applicant\n",
        "history['sec_app_earliest_cr_line'] = pd.to_datetime(history['sec_app_earliest_cr_line'], infer_datetime_format=True, errors='coerce')\n",
        "history['sec_app_earliest_cr_line'] = history['issue_d'] - history['sec_app_earliest_cr_line']\n",
        "history['sec_app_earliest_cr_line'] = history['sec_app_earliest_cr_line'].dt.days\n",
        "\n",
        "current['sec_app_earliest_cr_line'] = pd.to_datetime(current['sec_app_earliest_cr_line'], infer_datetime_format=True, errors='coerce')\n",
        "current['sec_app_earliest_cr_line'] = pd.Timestamp.today() - current['sec_app_earliest_cr_line']\n",
        "current['sec_app_earliest_cr_line'] = current['sec_app_earliest_cr_line'].dt.days\n",
        "\n",
        "# Engineer features for issue date year & month\n",
        "history['issue_d_year'] = history['issue_d'].dt.year\n",
        "history['issue_d_month'] = history['issue_d'].dt.month\n",
        "\n",
        "current['issue_d_year'] = pd.Timestamp.today().year\n",
        "current['issue_d_month'] = pd.Timestamp.today().month"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hbgClUDCR2M4",
        "colab": {}
      },
      "source": [
        "# Use Python sets to compare the historical columns & current columns\n",
        "\n",
        "common_columns = set(history.columns) & set(current.columns)\n",
        "just_history = set(history.columns) - set(current.columns)\n",
        "just_current = set(current.columns) - set(history.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s9e_llYWR2M6",
        "colab": {}
      },
      "source": [
        "# Train on the historical data.\n",
        "# For features, use only the common columns shared by the historical & current data.\n",
        "# For the target, use `loan_status` ('Fully Paid' or 'Charged Off')\n",
        "\n",
        "features = list(common_columns)\n",
        "target = 'loan_status'\n",
        "X = history[features]\n",
        "y = history[target]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iOoxx1wWR2M8",
        "colab": {}
      },
      "source": [
        "# Do train/validate/test 3-way split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y, test_size=20000, stratify=y, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval, test_size=20000, \n",
        "    stratify=y_trainval, random_state=42)\n",
        "\n",
        "print('X_train shape', X_train.shape)\n",
        "print('y_train shape', y_train.shape)\n",
        "print('X_val shape', X_val.shape)\n",
        "print('y_val shape', y_val.shape)\n",
        "print('X_test shape', X_test.shape)\n",
        "print('y_test shape', y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aFLirxfHR2M-"
      },
      "source": [
        "## Understand why accuracy is a misleading metric when classes are imbalanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0-KxM9aJR2M-"
      },
      "source": [
        "### Get accuracy score for majority class baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZRu7VrJpR2M_",
        "colab": {}
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ScnJsYSDeEu8",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "majority_class = y_train.mode()[0]\n",
        "y_pred = np.full_like(y_val, fill_value=majority_class)\n",
        "accuracy_score(y_val, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KHFJ4dBuR2NA"
      },
      "source": [
        "### Get confusion matrix for majority class baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HbQaHeabR2NA",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    labels = unique_labels(y_true)\n",
        "    columns = [f'Predicted {label}' for label in labels]\n",
        "    index = [f'Actual {label}' for label in labels]\n",
        "    table = pd.DataFrame(confusion_matrix(y_true, y_pred), \n",
        "                         columns=columns, index=index)\n",
        "    return sns.heatmap(table, annot=True, fmt='d', cmap='viridis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "snBr6NkLR2NC",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(y_val, y_pred);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "afQOq2ndR2ND"
      },
      "source": [
        "### Get precision & recall for majority class baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hrf7NHWbR2NE",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wA9UIWr0R2NF"
      },
      "source": [
        "### Get ROC AUC score for majority class baseline\n",
        "[sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xOe_uTZcR2NF",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# What if we predicted 100% probability of the positive class for every prediction?\n",
        "# This is like the majority class baseline, but with predicted probabilities,\n",
        "# instead of just discrete classes.\n",
        "# VERY IMPORTANT ‚Äî¬†Use predicted probabilities with ROC AUC score!\n",
        "# Because, it's a metric of how well you rank/sort predicted probabilities.\n",
        "y_pred_proba = np.full_like(y_val, fill_value=1.00)\n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vydmoSBchPS7",
        "colab": {}
      },
      "source": [
        "# ROC AUC is 0.50 by definition when predicting any constant probability value\n",
        "y_pred_proba = np.full_like(y_val, fill_value=0)\n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hjK8o9ZChkeV",
        "colab": {}
      },
      "source": [
        "y_pred_proba = np.full_like(y_val, fill_value=0.50)\n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peDsUqEYyFuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_val.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50tnSds0yFuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot ROC curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_val=='Charged Off', y_pred_proba)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4DiIg0-_R2NH"
      },
      "source": [
        "### Fit a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w1USCwFnR2NH"
      },
      "source": [
        "#### Count missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-mrGaDCR2NI",
        "colab": {}
      },
      "source": [
        "null_counts = X_train.isnull().sum().sort_values(ascending=False)\n",
        "null_counts.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XxgDKuQuR2NJ",
        "colab": {}
      },
      "source": [
        "many_nulls = null_counts[:73].index\n",
        "print(list(many_nulls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7puVifaPR2NM"
      },
      "source": [
        "#### Wrangle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hO2F3AdtR2NM",
        "colab": {}
      },
      "source": [
        "def wrangle(X):\n",
        "    X = X.copy()\n",
        "\n",
        "    # Engineer new feature for every feature: is the feature null?\n",
        "    for col in X:\n",
        "        X[col+'_NULL'] = X[col].isnull()\n",
        "    \n",
        "    # Convert percentages from strings to floats\n",
        "    X['int_rate'] = X['int_rate'].str.strip('%').astype(float)\n",
        "    X['revol_util'] = X['revol_util'].str.strip('%').astype(float)\n",
        "    \n",
        "    # Convert employment length from string to float\n",
        "    X['emp_length'] = X['emp_length'].str.replace(r'\\D','').astype(float)\n",
        "        \n",
        "    # Create features for three employee titles: teacher, manager, owner\n",
        "    X['emp_title'] = X['emp_title'].str.lower()\n",
        "    X['emp_title_teacher'] = X['emp_title'].str.contains('teacher', na=False)\n",
        "    X['emp_title_manager'] = X['emp_title'].str.contains('manager', na=False)\n",
        "    X['emp_title_owner']   = X['emp_title'].str.contains('owner', na=False)\n",
        "\n",
        "    # Get length of free text fields\n",
        "    X['title'] = X['title'].str.len()\n",
        "    X['desc'] = X['desc'].str.len()\n",
        "    X['emp_title'] = X['emp_title'].str.len()\n",
        "    \n",
        "    # Convert sub_grade from string \"A1\"-\"D5\" to integer 1-20\n",
        "    sub_grade_ranks = {'A1': 1, 'A2': 2, 'A3': 3, 'A4': 4, 'A5': 5, 'B1': 6, 'B2': 7, \n",
        "                       'B3': 8, 'B4': 9, 'B5': 10, 'C1': 11, 'C2': 12, 'C3': 13, 'C4': 14, \n",
        "                       'C5': 15, 'D1': 16, 'D2': 17, 'D3': 18, 'D4': 19, 'D5': 20}\n",
        "    X['sub_grade'] = X['sub_grade'].map(sub_grade_ranks)\n",
        "    \n",
        "    # Drop some columns\n",
        "    X = X.drop(columns='id')        # Always unique\n",
        "    X = X.drop(columns='url')       # Always unique\n",
        "    X = X.drop(columns='member_id') # Always null\n",
        "    X = X.drop(columns='grade')     # Duplicative of sub_grade\n",
        "    X = X.drop(columns='zip_code')  # High cardinality\n",
        "    \n",
        "    # Only use these features which had nonzero permutation importances in earlier models    \n",
        "    features = ['acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc', \n",
        "                'annual_inc_joint', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', \n",
        "                'collections_12_mths_ex_med', 'delinq_amnt', 'desc_NULL', 'dti', \n",
        "                'dti_joint', 'earliest_cr_line', 'emp_length', 'emp_length_NULL', \n",
        "                'emp_title', 'emp_title_NULL', 'emp_title_owner', 'fico_range_high', \n",
        "                'funded_amnt', 'home_ownership', 'inq_last_12m', 'inq_last_6mths', \n",
        "                'installment', 'int_rate', 'issue_d_month', 'issue_d_year', 'loan_amnt', \n",
        "                'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', \n",
        "                'mo_sin_rcnt_rev_tl_op', 'mort_acc', 'mths_since_last_major_derog_NULL', \n",
        "                'mths_since_last_record', 'mths_since_recent_bc', 'mths_since_recent_inq', \n",
        "                'num_actv_bc_tl', 'num_actv_rev_tl', 'num_op_rev_tl', 'num_rev_tl_bal_gt_0', \n",
        "                'num_tl_120dpd_2m_NULL', 'open_rv_12m_NULL', 'open_rv_24m', \n",
        "                'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'purpose', \n",
        "                'revol_bal', 'revol_bal_joint', 'sec_app_earliest_cr_line', \n",
        "                'sec_app_fico_range_high', 'sec_app_open_acc', 'sec_app_open_act_il', \n",
        "                'sub_grade', 'term', 'title', 'title_NULL', 'tot_coll_amt', \n",
        "                'tot_hi_cred_lim', 'total_acc', 'total_bal_il', 'total_bc_limit', \n",
        "                'total_cu_tl', 'total_rev_hi_lim']    \n",
        "    X = X[features]\n",
        "    \n",
        "    # Return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "\n",
        "X_train = wrangle(X_train)\n",
        "X_val   = wrangle(X_val)\n",
        "X_test  = wrangle(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlFxLAnrR2NO",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "import category_encoders as ce\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='median'), \n",
        "    RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ihp_kbdGR2NQ"
      },
      "source": [
        "### Get accuracy score for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDqQXwPCR2NQ",
        "colab": {}
      },
      "source": [
        "y_pred = pipeline.predict(X_val)\n",
        "accuracy_score(y_val, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-GootfQvR2NU"
      },
      "source": [
        "### Get confusion matrix for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vCDL4JA1R2NU",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(y_val, y_pred);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FaaOcOxQR2NW"
      },
      "source": [
        "### Get precision & recall for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JiNdsscoR2NX",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VbTRfBJRR2NZ"
      },
      "source": [
        "### Get ROC AUC score for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OOWwRVGOR2Na",
        "colab": {}
      },
      "source": [
        "y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vPv8SuZMR2Nb"
      },
      "source": [
        "## Understand ROC AUC (Receiver Operating Characteristic, Area Under the Curve)\n",
        "\n",
        "#### Scikit-Learn docs\n",
        "- [User Guide: Receiver operating characteristic (ROC)](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc)\n",
        "- [sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
        "- [sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
        "\n",
        "#### More links\n",
        "- [ROC curves and Area Under the Curve explained](https://www.dataschool.io/roc-curves-and-auc-explained/)\n",
        "- [The philosophical argument for using ROC curves](https://lukeoakdenrayner.wordpress.com/2018/01/07/the-philosophical-argument-for-using-roc-curves/)\n",
        "\n",
        "[Wikipedia explains,](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) \"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\"\n",
        "\n",
        "ROC AUC is the area under the ROC curve. [It can be interpreted](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it) as \"the expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative.\" \n",
        "\n",
        "ROC AUC measures how well a classifier ranks predicted probabilities. It ranges from 0 to 1. A naive majority class baseline will have an ROC AUC score of 0.5. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nBhPT1FHR2Nb"
      },
      "source": [
        "## Visualize the ROC curve by plotting true positive rate vs false positive rate at varying thresholds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ytNvg-wsR2Nc",
        "colab": {}
      },
      "source": [
        "from ipywidgets import interact, fixed\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def set_threshold(y_true, y_pred_proba, threshold=0.5):\n",
        "    \"\"\"\n",
        "    For binary classification problems. \n",
        "    y_pred_proba : predicted probability of class 1\n",
        "    \"\"\"\n",
        "    \n",
        "    # Apply threshold to predicted probabilities\n",
        "    # to get discrete predictions\n",
        "    class_0, class_1 = unique_labels(y_true)\n",
        "    y_pred = np.full_like(y_true, fill_value=class_0)\n",
        "    y_pred[y_pred_proba > threshold] = class_1\n",
        "    \n",
        "    # Plot distribution of predicted probabilities\n",
        "    ax = sns.distplot(y_pred_proba)\n",
        "    ax.axvline(threshold, color='red')\n",
        "    plt.title('Distribution of predicted probabilities')\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate true positive rate and false positive rate\n",
        "    true_positives = (y_pred==y_true) & (y_pred==class_1)\n",
        "    false_positives = (y_pred!=y_true) & (y_pred==class_1)\n",
        "    actual_positives = (y_true==class_1)\n",
        "    actual_negatives = (y_true==class_0)\n",
        "    true_positive_rate = true_positives.sum() / actual_positives.sum()\n",
        "    false_positive_rate = false_positives.sum() / actual_negatives.sum()\n",
        "    print('False Positive Rate', false_positive_rate)\n",
        "    print('True Positive Rate', true_positive_rate)\n",
        "    \n",
        "    # Plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_true==class_1, y_pred_proba)\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.title('ROC curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    \n",
        "    # Plot point on ROC curve for the current threshold\n",
        "    plt.scatter(false_positive_rate, true_positive_rate)\n",
        "    plt.show()\n",
        "    \n",
        "    # Show ROC AUC score\n",
        "    print('Area under the Receiver Operating Characteristic curve:', \n",
        "          roc_auc_score(y_true, y_pred_proba))\n",
        "    \n",
        "    # Show confusion matrix & classification report\n",
        "    plot_confusion_matrix(y_true, y_pred)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "interact(set_threshold, \n",
        "         y_true=fixed(y_val), \n",
        "         y_pred_proba=fixed(y_pred_proba), \n",
        "         threshold=(0,1,0.05));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DWBVRiZdR2Nd"
      },
      "source": [
        "### Use the class_weight parameter in scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XTcCfxvgR2Nd"
      },
      "source": [
        "Here's a fun demo you can explore! The next code cells do five things:\n",
        "\n",
        "#### 1. Generate data\n",
        "\n",
        "We use scikit-learn's [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function to generate fake data for a binary classification problem, based on several parameters, including:\n",
        "- Number of samples\n",
        "- Weights, meaning \"the proportions of samples assigned to each class.\"\n",
        "- Class separation: \"Larger values spread out the clusters/classes and make the classification task easier.\"\n",
        "\n",
        "(We are generating fake data so it is easy to visualize.)\n",
        "\n",
        "#### 2. Split data\n",
        "\n",
        "We split the data three ways, into train, validation, and test sets. (For this toy example, it's not really necessary to do a three-way split. A two-way split, or even no split, would be ok. But I'm trying to demonstrate good habits, even in toy examples, to avoid confusion.)\n",
        "\n",
        "#### 3. Fit model\n",
        "\n",
        "We use scikit-learn to fit a [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on the training data.\n",
        "\n",
        "We use this model parameter:\n",
        "\n",
        "> **class_weight : _dict or ‚Äòbalanced‚Äô, default: None_**\n",
        "\n",
        "> Weights associated with classes in the form `{class_label: weight}`. If not given, all classes are supposed to have weight one.\n",
        "\n",
        "> The ‚Äúbalanced‚Äù mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`.\n",
        "\n",
        "\n",
        "#### 4. Evaluate model\n",
        "\n",
        "We use our Logistic Regression model, which was fit on the training data, to generate predictions for the validation data.\n",
        "\n",
        "Then we print [scikit-learn's Classification Report](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report), with many metrics, and also the accuracy score. We are comparing the correct labels to the Logistic Regression's predicted labels, for the validation set. \n",
        "\n",
        "#### 5. Visualize decision function\n",
        "\n",
        "Based on these examples\n",
        "- https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html\n",
        "- http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/#example-1-decision-regions-in-2d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8G277Q2CR2Ne",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_validation_test_split(\n",
        "    X, y, train_size=0.8, val_size=0.1, test_size=0.1, \n",
        "    random_state=None, shuffle=True):\n",
        "        \n",
        "    assert train_size + val_size + test_size == 1\n",
        "    \n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, shuffle=shuffle)\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=val_size/(train_size+val_size), \n",
        "        random_state=random_state, shuffle=shuffle)\n",
        "    \n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a3NHQGQ_R2Ng",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "\n",
        "#1. Generate data\n",
        "\n",
        "# Try re-running the cell with different values for these parameters\n",
        "n_samples = 1000\n",
        "weights = (0.95, 0.05)\n",
        "class_sep = 0.8\n",
        "\n",
        "X, y = make_classification(n_samples=n_samples, n_features=2, n_informative=2, \n",
        "                           n_redundant=0, n_repeated=0, n_classes=2, \n",
        "                           n_clusters_per_class=1, weights=weights, \n",
        "                           class_sep=class_sep, random_state=0)\n",
        "\n",
        "\n",
        "# 2. Split data\n",
        "\n",
        "# Uses our custom train_validation_test_split function\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n",
        "    X, y, train_size=0.8, val_size=0.1, test_size=0.1, random_state=1)\n",
        "\n",
        "\n",
        "# 3. Fit model\n",
        "\n",
        "# Try re-running the cell with different values for this parameter\n",
        "class_weight = {0: 1, 1: 1}\n",
        "\n",
        "model = LogisticRegression(solver='lbfgs', class_weight=class_weight)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# 4. Evaluate model\n",
        "\n",
        "y_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))\n",
        "plot_confusion_matrix(y_val, y_pred)\n",
        "\n",
        "# 5. Visualize decision regions\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_decision_regions(X_val, y_val, model, legend=0);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P3rGNwAjR2Nh"
      },
      "source": [
        "Try re-running the cell above with different values for these four parameters:\n",
        "- `n_samples`\n",
        "- `weights`\n",
        "- `class_sep`\n",
        "- `class_balance`\n",
        "\n",
        "For example, with a 50% / 50% class distribution:\n",
        "```\n",
        "n_samples = 1000\n",
        "weights = (0.50, 0.50)\n",
        "class_sep = 0.8\n",
        "class_balance = None\n",
        "```\n",
        "\n",
        "With a 95% / 5% class distribution:\n",
        "```\n",
        "n_samples = 1000\n",
        "weights = (0.95, 0.05)\n",
        "class_sep = 0.8\n",
        "class_balance = None\n",
        "```\n",
        "\n",
        "With the same 95% / 5% class distribution, but changing the Logistic Regression's `class_balance` parameter to `'balanced'` (instead of its default `None`)\n",
        "```\n",
        "n_samples = 1000\n",
        "weights = (0.95, 0.05)\n",
        "class_sep = 0.8\n",
        "class_balance = 'balanced'\n",
        "```\n",
        "\n",
        "With the same 95% / 5% class distribution, but with different values for `class_balance`:\n",
        "- `{0: 1, 1: 1}` _(equivalent to `None`)_\n",
        "- `{0: 1, 1: 2}`\n",
        "- `{0: 1, 1: 10}` _(roughly equivalent to `'balanced'` for this dataset)_\n",
        "- `{0: 1, 1: 100}`\n",
        "- `{0: 1, 1: 10000}`\n",
        "\n",
        "How do the evaluation metrics and decision region plots change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9GhisE6vR2Ni"
      },
      "source": [
        "## What you can do about imbalanced classes\n",
        "\n",
        "[Learning from Imbalanced Classes](https://www.svds.com/tbt-learning-imbalanced-classes/) gives \"a rough outline of useful approaches\" : \n",
        "\n",
        "- Do nothing. Sometimes you get lucky and nothing needs to be done. You can train on the so-called natural (or stratified) distribution and sometimes it works without need for modification.\n",
        "- Balance the training set in some way:\n",
        "  - Oversample the minority class.\n",
        "  - Undersample the majority class.\n",
        "  - Synthesize new minority classes.\n",
        "- Throw away minority examples and switch to an anomaly detection framework.\n",
        "- At the algorithm level, or after it:\n",
        "  - Adjust the class weight (misclassification costs).\n",
        "  - Adjust the decision threshold.\n",
        "  - Modify an existing algorithm to be more sensitive to rare classes.\n",
        "- Construct an entirely new algorithm to perform well on imbalanced data.\n",
        "\n",
        "#### We demonstrated two of these options: \n",
        "\n",
        "- \"Adjust the class weight (misclassification costs)\" ‚Äî many scikit-learn classifiers have a `class_balance` parameter\n",
        "- \"Adjust the decision threshold\" ‚Äî you can lean more about this in a great blog post, [Visualizing Machine Learning Thresholds to Make Better Business Decisions](https://blog.insightdatascience.com/visualizing-machine-learning-thresholds-to-make-better-business-decisions-4ab07f823415).\n",
        "\n",
        "#### Another option to be aware of:\n",
        "- The [imbalance-learn](https://github.com/scikit-learn-contrib/imbalanced-learn) library can be used to \"oversample the minority class, undersample the majority class, or synthesize new minority classes.\""
      ]
    }
  ]
}