{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_applied_modeling_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCc3XZEyG3XV",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science, Unit 2: Predictive Modeling\n",
        "\n",
        "# Applied Modeling, Module 4\n",
        "\n",
        "You will use your portfolio project dataset for all assignments this sprint.\n",
        "\n",
        "## Assignment\n",
        "\n",
        "Complete these tasks for your project, and document your work.\n",
        "\n",
        "- [ ] Continue to iterate on your project: data cleaning, exploratory visualization, feature engineering, modeling.\n",
        "- [ ] Make a Shapley force plot to explain at least 1 individual prediction.\n",
        "- [ ] Share at least 1 visualization on Slack.\n",
        "\n",
        "(If you haven't completed an initial model yet for your portfolio project, then do today's assignment using your Tanzania Waterpumps model.)\n",
        "\n",
        "## Stretch Goals\n",
        "- [ ] Make Shapley force plots to explain at least 4 individual predictions.\n",
        "    - If your project is Binary Classification, you can do a True Positive, True Negative, False Positive, False Negative.\n",
        "    - If your project is Regression, you can do a high prediction with low error, a low prediction with low error, a high prediction with high error, and a low prediction with high error.\n",
        "- [ ] Use Shapley values to display verbal explanations of individual predictions.\n",
        "- [ ] Use the SHAP library for other visualization types.\n",
        "\n",
        "The [SHAP repo](https://github.com/slundberg/shap) has examples for many visualization types, including:\n",
        "\n",
        "- Force Plot, individual predictions\n",
        "- Force Plot, multiple predictions\n",
        "- Dependence Plot\n",
        "- Summary Plot\n",
        "- Summary Plot, Bar\n",
        "- Interaction Values\n",
        "- Decision Plots\n",
        "\n",
        "We just did the first type during the lesson. The [Kaggle microcourse](https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values) shows two more. Experiment and see what you can learn!\n",
        "\n",
        "\n",
        "## Links\n",
        "- [Kaggle / Dan Becker: Machine Learning Explainability — SHAP Values](https://www.kaggle.com/learn/machine-learning-explainability)\n",
        "- [Christoph Molnar: Interpretable Machine Learning — Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html)\n",
        "- [SHAP repo](https://github.com/slundberg/shap) & [docs](https://shap.readthedocs.io/en/latest/)"
      ]
    }
  ]
}