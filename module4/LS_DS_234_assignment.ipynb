{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "nCc3XZEyG3XV"}, "source": ["Lambda School Data Science\n", "\n", "*Unit 2, Sprint 3, Module 4*\n", "\n", "---\n", "\n", "\n", "# Model Interpretation 2\n", "\n", "You will use your portfolio project dataset for all assignments this sprint.\n", "\n", "## Assignment\n", "\n", "Complete these tasks for your project, and document your work.\n", "\n", "- [ ] Continue to iterate on your project: data cleaning, exploratory visualization, feature engineering, modeling.\n", "- [ ] Make a Shapley force plot to explain at least 1 individual prediction.\n", "- [ ] Share at least 1 visualization (of any type) on Slack.\n", "\n", "But, if you aren't ready to make a Shapley force plot with your own dataset today, that's okay. You can practice this objective with another dataset instead. You may choose any dataset you've worked with previously.\n", "\n", "## Stretch Goals\n", "- [ ] Make Shapley force plots to explain at least 4 individual predictions.\n", "    - If your project is Binary Classification, you can do a True Positive, True Negative, False Positive, False Negative.\n", "    - If your project is Regression, you can do a high prediction with low error, a low prediction with low error, a high prediction with high error, and a low prediction with high error.\n", "- [ ] Use Shapley values to display verbal explanations of individual predictions.\n", "- [ ] Use the SHAP library for other visualization types.\n", "\n", "The [SHAP repo](https://github.com/slundberg/shap) has examples for many visualization types, including:\n", "\n", "- Force Plot, individual predictions\n", "- Force Plot, multiple predictions\n", "- Dependence Plot\n", "- Summary Plot\n", "- Summary Plot, Bar\n", "- Interaction Values\n", "- Decision Plots\n", "\n", "We just did the first type during the lesson. The [Kaggle microcourse](https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values) shows two more. Experiment and see what you can learn!\n", "\n", "\n", "## Links\n", "- [Kaggle / Dan Becker: Machine Learning Explainability \u2014\u00a0SHAP Values](https://www.kaggle.com/learn/machine-learning-explainability)\n", "- [Christoph Molnar: Interpretable Machine Learning \u2014 Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html)\n", "- [SHAP repo](https://github.com/slundberg/shap) & [docs](https://shap.readthedocs.io/en/latest/)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%capture\n", "import sys\n", "\n", "# If you're on Colab:\n", "if 'google.colab' in sys.modules:\n", "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n", "    !pip install category_encoders==2.*\n", "    !pip install eli5\n", "    !pip install pdpbox\n", "    !pip install shap\n", "\n", "# If you're working locally:\n", "else:\n", "    DATA_PATH = '../data/'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 1}