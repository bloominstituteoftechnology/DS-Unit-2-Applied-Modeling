{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_424_Deploy_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "py37  (Python3)",
      "language": "python",
      "name": "py37"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laguz/DS-Unit-2-Applied-Modeling/blob/master/LS_DS_424_Deploy_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# *Data Science Unit 4 Sprint 2 Assignment 4*\n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Apply regularization techniques to your model. \n",
        "\n",
        "**Don't forgot to switch to GPU on Colab!**\n",
        "\n",
        "\n",
        "## Objective \n",
        "\n",
        "In lecture, you were exposed to Lp space reguarlization, Max Norn weight constraints, and dropout. \n",
        "\n",
        "In this assignment, you will run several experiments in order to perform a deeper analysis on the effects that various regularization techniques have model performance and on the learned model weights. \n",
        "\n",
        "By the end of this assignment, these regularization techniques should no longer feel like black boxes to you (i.e. completely mysterious as to how they work.) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptJ2b3wk62Ud"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# native libraries \n",
        "import os\n",
        "from time import time \n",
        "\n",
        "# data analysis libraries \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# deep learning libraries \n",
        "from keras import Sequential\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "from keras.layers import Flatten, Dense, Dropout\n",
        "from keras.layers import ReLU\n",
        "from keras.initializers import GlorotUniform\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# regularizers \n",
        "from keras.regularizers import l2, l1\n",
        "from keras.constraints import MaxNorm\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# native python unit test library\n",
        "from unittest import TestCase\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ch9ay3aw2P0"
      },
      "source": [
        "-----\n",
        "\n",
        "# $L_p$ Space Regularization \n",
        "\n",
        "## Bridging Theory and Practice \n",
        "\n",
        "Because the idea of infinitely many vector spaces, each with their very own distance metric for measuring distance differently can seem very abstract, we are going to take that distance metric general formula and look at a few special cases by writing custom regularization functions and taking note of their effect a the model's learning outcomes. \n",
        "\n",
        "Don't forget to review the theory of $L_p$ Space Regularization in the guided project. \n",
        "\n",
        "Also, watch this video if you haven't already. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1KXE9jow2P0"
      },
      "source": [
        "# check out this video for an animated explaination of Lp Space and distance metrics \n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('FiSy6zWDfiA', width=800, height=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sercimjcw2P1"
      },
      "source": [
        "### Distance Metric General Formula\n",
        "$${\\displaystyle \\left\\|x\\right\\|_{p}=\\left(|x_{1}|^{p}+|x_{2}|^{p}+\\dotsb +|x_{n}|^{p}\\right)^{1/p}.}$$\n",
        "\n",
        "Let's create a class for the distance metric general formula.\n",
        "\n",
        "\n",
        "There are 2 classes below: \n",
        "\n",
        "```python\n",
        "class Test_distance_metric_solution()\n",
        "```\n",
        "\n",
        "You don't need to change anything in this class. This class is here in order to make sure that you calculate each portion of the distance metric general formula correctly. \n",
        "\n",
        "\n",
        "```python\n",
        "class Lp_distance_metric_general_formula()\n",
        "```\n",
        "\n",
        "This is the class that you will complete. \n",
        "\n",
        "Each of the non `__call__` methods calculate one portion of the distance metric general formula. \n",
        "\n",
        "Breaking up each portion of the formula into a separate method is actually unnecessary. \n",
        "\n",
        "Including the entire calculation of the general formula into a single method is ideal, however it is more difficult to write granular unit tests that way. \n",
        "\n",
        "So, for instructional purposes, it was broken up into 3 methods. However, outside of an academic environment, we would only need the `__init__` and `__call__` methods. \n",
        "\n",
        "Having said that, it is good for you to see how a custom unit test class is used to test the calculations of another class. This is a portion of what software engineering looks like. Get used to it. You'll need good software engineering practices to make it in the world. You'll learn more about this in your CS unit. For now, consider it a bit of foreshadowing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gya6GahFw2P2"
      },
      "source": [
        "class Test_distance_metric_solution(TestCase):\n",
        "    \"\"\"\n",
        "    This is a unit test class desgined to make sure that the student calcualtes each component \n",
        "    of the Distance Metric General Formula correctly. Thereby getting real-time feedback and correction. \n",
        "    \"\"\"\n",
        "    \n",
        "    def test_squared_vector_comps(self, test_array):\n",
        "        \"\"\"\n",
        "        This test makes sure that student calculate the sum_of_squared_comp correctly. \n",
        "        \"\"\"\n",
        "        answer = np.array([1., 4.])\n",
        "        # error message in case if test case got failed\n",
        "        message = \"you did not calcualte sum_of_squared_comp correctly\"\n",
        "        # assert function() to check if values are almost equal\n",
        "        np.testing.assert_array_equal(answer, test_array,  err_msg=message)\n",
        "        \n",
        "        \n",
        "    def test_sum_of_squared_comp(self, test_sum):\n",
        "        \"\"\"\n",
        "        This test makes sure that student calculate the sum_of_squared_comp correctly. \n",
        "        \"\"\"\n",
        "        answer = 5\n",
        "        # error message in case if test case got failed\n",
        "        message = \"you did not calcualte sum_of_squared_comp correctly\"\n",
        "        # assert function() to check if values are almost equal\n",
        "        np.testing.assert_array_equal(answer, test_sum,  err_msg=message)\n",
        "        \n",
        "    def test_vector_norm(self, test_norm):\n",
        "        \"\"\"\n",
        "        This test makes sure that student calculate the vector_norm correctly. \n",
        "        \"\"\"\n",
        "        answer = 2.236\n",
        "        decimalPlace = 3\n",
        "        # error message in case if test case got failed\n",
        "        message = \"you did not calcualte sum_of_squared_comp correctly\"\n",
        "        # assert function() to check if values are almost equal\n",
        "        self.assertAlmostEqual(answer, test_norm,  decimalPlace, message)        "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF3TI6_Yw2P3"
      },
      "source": [
        "### Fill in the missing code in the class below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b8830110178a58a6fe1973906f649d36",
          "grade": false,
          "grade_id": "cell-0bd59a2d10fbf254",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "fSmmSJxFw2P4"
      },
      "source": [
        "class Lp_distance_metric_general_formula(object):\n",
        "    \"\"\"\n",
        "    This class takes the Lp distance metric general formual and sets p equal to a value provided by the user. \n",
        "    This has the effect of deriving a distance metric for a specific metric space and calcualting the distance \n",
        "    of a vector in that metric space.\n",
        "    \n",
        "    Example\n",
        "    -------\n",
        "    If the user sets p = 2, then the euclidean distance formula is derived.\n",
        "    If the user sets p = 1, then the taxicab distance formula is derived. \n",
        "    \n",
        "    Note\n",
        "    ----\n",
        "    It is possible to use p values less than 1 but those are special cases that we will ignore. \n",
        "    These special values are interesting for academic purposes but in practice you very likely won't need to know \n",
        "    about them.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=2, reg_strength = 1.0):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        \n",
        "        p: int or float\n",
        "            p value used for calculating the distance of a vector in a certain metric space\n",
        "            \n",
        "        reg_strength: int or float\n",
        "            usually set to a value less than 1.0 to decrease the strength of the distance metric when used as a model regularizer\n",
        "            keep this value at 1.0 when measureing vector norms (i.e. vector lengths)\n",
        "        \"\"\"\n",
        "        \n",
        "        assert p >=1 , \"p value must be greater than or equal to 1\"\n",
        "        \n",
        "        self.p = p\n",
        "        self.reg_strength = reg_strength\n",
        "                \n",
        "    def calc_squared_vector_comps(self):\n",
        "    \n",
        "        # raise each vector component in self.x to the power of p\n",
        "        # save result to self.squared_vector_comps\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    def calc_sum_of_squared_comp(self):\n",
        "        # take the sum of the squared components in self.squared_vector_comps\n",
        "        # save to self.sum_of_squared_comp\n",
        "        # hint: use tf.reduce_sum\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    def calc_vector_norm(self):\n",
        "        \n",
        "        # take the 1/p root of the self.sum_of_squared_comp in order to calculate the norm, i.e. ||x||\n",
        "        # save result to self.vector_norm\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "        \n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        This method calcualtes the distance (i.e. norm) for vector x in Lp space for a value p given by the use\n",
        "        \n",
        "        ‚Äñùë•‚Äñùëù = (|ùë•_1|^ùëù + |ùë•_2|^ùëù + ‚ãØ +|ùë•_ùëõ|^ùëù )^1/ùëù\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x: N-dimsional numpy array or tensorflow tensor of floats \n",
        "            x is our vector, could be a weight vector but any vector is valid \n",
        "            \n",
        "            \n",
        "        HINT\n",
        "        -----\n",
        "        You must use self.p when calculating squared_vector_comps and vector_norm\n",
        "        \"\"\"\n",
        "        \n",
        "        self.x = x\n",
        "        \n",
        "        # calculate these parts |ùë•_i|^ùëù\n",
        "        self.calc_squared_vector_comps()\n",
        "        \n",
        "        # calcualte this |ùë•_1|^ùëù + |ùë•_2|^ùëù + ‚ãØ +|ùë•_ùëõ|^ùëù\n",
        "        self.calc_sum_of_squared_comp()\n",
        "        \n",
        "        # calculate this (|ùë•_1|^ùëù + |ùë•_2|^ùëù + ‚ãØ +|ùë•_ùëõ|^ùëù )^1/ùëù\n",
        "        self.calc_vector_norm()\n",
        "        \n",
        "        # return the vector norm scaled by a regularization penality\n",
        "        # we say penality because the value is usually less than 1.0 thereby scaling down the norm\n",
        "        return self.reg_strength * self.vector_norm"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrfC3z_Tw2P6"
      },
      "source": [
        "\n",
        "### Unit Test your code\n",
        "\n",
        "You know you wrote your class correctly when all the unit tests pass. \n",
        "\n",
        "So the code in the following cell should run without throwing a single error. \n",
        "\n",
        "Protip: you can comment out 2nd and 3rd unit test in order to test your results for the first method and then uncomment as you go. Feel free to create a new cell and run your own testing there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_HTDwYYw2P7"
      },
      "source": [
        "# Did see this stuff in class or in the module notes. I know you want me to learn but this is just like univerisity class teach you how to do something super easy and give something super hard\n",
        "#that you never saw for homework. \n",
        "\n",
        "# instantiate the unit test class that will check the calculates of Lp_distance_metric_general_formula's methods\n",
        "tests = Test_distance_metric_solution()\n",
        "\n",
        "# instantiate Lp_distance_metric_general_formula, set p = 2 in order to derive the euclidean distance metric\n",
        "lp = Lp_distance_metric_general_formula( p=2, reg_strength = 1.0)\n",
        "\n",
        "# don't change this test_vector\n",
        "# Test_distance_metric_solution assumes that you're using ths exact test_vector\n",
        "test_vector = np.array([1., 2.])\n",
        "#lp(test_vector)\n",
        "\n",
        "# test the calculations that are perform in each of the following lp class methods \n",
        "#tests.test_squared_vector_comps(lp.squared_vector_comps)\n",
        "#tests.test_sum_of_squared_comp(lp.sum_of_squared_comp)\n",
        "#tests.test_vector_norm(lp.vector_norm.numpy())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXFtRqugw2P8"
      },
      "source": [
        "# Did see this stuff in class or in the module notes. I know you want me to learn but this is just like univerisity class teach you how to do something super easy and give something super hard\n",
        "#that you never saw for homework. \n",
        "# note: because we use tf.reduce_sum to calculate sum_of_squared_comp\n",
        "# the result is inside of a tensor and we need to use .numpy() to get the scalar out of the tensor \n",
        "#print (lp.vector_norm)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvhMj4Tew2QD"
      },
      "source": [
        "# Did see this stuff in class or in the module notes. I know you want me to learn but this is just like univerisity class teach you how to do something super easy and give something super hard\n",
        "#that you never saw for homework. \n",
        "#print (lp.vector_norm.numpy())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmDSGzRAw2QD"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV8thOLow2QE"
      },
      "source": [
        "### Apply our $L_p$ Space Class\n",
        "\n",
        "Next we will use our distance metric class in order to calculate the euclidean and taxicab distance of our vector below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "kecRnMBXw2QE",
        "outputId": "55c48e3a-ee0d-46d9-8840-ceb93d511463"
      },
      "source": [
        "# W is a 2D vector with an x and y component, i.e. (x, y) = (4,3)\n",
        "w = np.array([4., 3.])\n",
        "\n",
        "# origin point - we need to specify the starting point for plotting\n",
        "origin = np.array([0,0])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = plt.axes()\n",
        "\n",
        "ax.arrow(origin[0], origin[1], w[0], w[1], head_width=0.5, head_length=0.7, fc='lightblue', ec='black')\n",
        "\n",
        "plt.grid()\n",
        "plt.yticks(np.arange(0,5))\n",
        "plt.xticks(np.arange(0,6));"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyBsmhqsw2QE"
      },
      "source": [
        "## Derive Euclidean Distance from the General Formula \n",
        "\n",
        "This is the general formula for distance metrics. Where we have an N-dimensional weight vector **w**. Notice that the general formula has a vector component for each of the N dimensions. Hence, it is the general formula.\n",
        "\n",
        "$${\\displaystyle \\left\\|\\textbf{w}\\right\\|_{p}=\\left(|w_{1}|^{p}+|w_{2}|^{p}+\\dotsb +|w_{n}|^{p}\\right)^{1/p}.}$$\n",
        " \n",
        "\n",
        "Let N = 2 such that our weight vector now exists in 2 dimensional space. \n",
        "\n",
        "Let p = 2 such that our weight vector's distance will be calculated in $L_{p=2}$ space \n",
        "\n",
        "In which case our general formula gets reduced from N-dimensions to 2-dimensions. So now we only need to consider a distance formula for a vector with 2 components, one for each dimension.\n",
        "\n",
        "$$||\\textbf{w}||_{p=2} = ((x_2 - x_1)^2 + (y_2 - y_1)^2)^{1/2}$$\n",
        "\n",
        "Now just re-express the square root and we arrive at the familiar Euclidean Distance from high school algebra. \n",
        "\n",
        "$$||\\textbf{w}||_{p=2} = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$$\n",
        "\n",
        "\n",
        "You might be wondering about why the general case doesn't have differences for the components but the Euclidean Distance does explicitly show the component wise differences. It is common in mathematics to suppress certain information, especially if that information is \"obvious\" - however what is obvious is highly relative. \n",
        "\n",
        "The general formula assumes that the vector starts at the origin, in which case, there's no need to show the subtraction of zero from a vector component. \n",
        "\n",
        "Assuming that our vector starts at the origin (and it does) the formula can be reduced to \n",
        "\n",
        "$$||\\textbf{w}||_{p=2} = \\sqrt{(x_2 - 0)^2 + (y_2 - 0)^2}$$\n",
        "\n",
        "$$||\\textbf{w}||_{p=2} = \\sqrt{x_2^2 + y_2^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBthkXUvw2QF"
      },
      "source": [
        "### Euclidean distance of our Vector \n",
        "\n",
        "With **p=2** in our `Lp_distance_metric_general_formula` class, we can calculate the euclidean distance of our vector **w**. I encourage you to either solve the euclidean distance of our vector either in your head or on paper to prove to yourself that the above formula that we derived works and gives you `5` as an answer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bf0cdea85e37c69f73956bf851cba8d6",
          "grade": false,
          "grade_id": "cell-74dae14f4bbad1ab",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Gh1vEfquw2QF"
      },
      "source": [
        "# Did see this stuff in class or in the module notes. I know you want me to learn but this is just like univerisity class teach you how to do something super easy and give something super hard\n",
        "#that you never saw for homework. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgN8S9YLw2QG"
      },
      "source": [
        "-----\n",
        "\n",
        "## Derive Taxicab Distance from the General Formula \n",
        "\n",
        "This is the general formula for distance metrics. Where we have an N-dimensional weight vector **w**. Notice that the general formula has a vector component for each of the N dimensions. Hence, it is the general formula.\n",
        "\n",
        "$${\\displaystyle \\left\\|\\textbf{w}\\right\\|_{p}=\\left(|w_{1}|^{p}+|w_{2}|^{p}+\\dotsb +|w_{n}|^{p}\\right)^{1/p}.}$$\n",
        " \n",
        "\n",
        "Let N = 2 such that our weight vector now exists in 2 dimensional space. \n",
        "\n",
        "Let p = 1 such that our weight vector's distance will be calculated in $L_{p=1}$ space \n",
        "\n",
        "\n",
        "In which case our general formula gets reduced from N-dimensions to 2-dimensions. So now we only need to consider a distance formula for a vector with 2 components, one for each dimension.\n",
        "\n",
        "$$||\\textbf{w}||_{p=1} = ((x_2 - x_1)^1 + (y_2 - y_1)^1)^{1/1}$$\n",
        "\n",
        "There's no need to express all those 1's\n",
        "\n",
        "$$||\\textbf{w}||_{p=1} = (x_2 - x_1) + (y_2 - y_1) $$\n",
        "\n",
        "Assuming that our vector starts at the origin (which it does)\n",
        "\n",
        "$$||\\textbf{w}||_{p=1} = (x_2 - 0) + (y_2 - 0) $$\n",
        "\n",
        "\n",
        "\n",
        "$$||\\textbf{w}||_{p=1} = x + y $$\n",
        "\n",
        "We have just derived the Taxicab distance metric from the general formula. The above equation tells us to add up all the steps in the x and y direction in order to calculate the Taxicab distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyM2hYeIw2QG"
      },
      "source": [
        "### Taxicab distance of our Vector \n",
        "\n",
        "With **p=1** in our `Lp_distance_metric_general_formula` class, we can calculate the taxicab distance of our vector **w**. This distance we can easily calculate in our head. Look at the plot of the vector and simply add up all the x and y components, i.e. count up all the steps you have to take in order to \"walk\" from the origin to the head of the vector. Do this to prove to yourself that the taxicab distance of our vector is `7`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9a527c635255f30979d1759e4ecc480d",
          "grade": false,
          "grade_id": "cell-9bf41d7142ad35e9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_-Bl7K3Gw2QH"
      },
      "source": [
        "# Did see this stuff in class or in the module notes. I know you want me to learn but this is just like univerisity class teach you how to do something super easy and give something super hard\n",
        "#that you never saw for homework. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJP3w0rw2QH"
      },
      "source": [
        "### Elastic Net Distance \n",
        "\n",
        "\n",
        "Elastic Net is a combination of L1 and L2. Compare the geometry below. \n",
        "\n",
        "![](https://ds100.org/sp17/assets/notebooks/linear_regression/norm_balls.png)\n",
        "\n",
        "The mathematical derivation of the distance metric for  $L_{p=3/2}$ space will be left to you as an optional exercise. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0f19be155b2e5be75c855710eab34cdd",
          "grade": false,
          "grade_id": "cell-d657c845dbb1ab23",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "35ovx-yFw2QH"
      },
      "source": [
        "# use Lp_distance_metric_general_formula class to calculate the Elastic Net distance of w\n",
        "# Did see this stuff in class or in the module notes. I know you want me to learn but this is just like univerisity class teach you how to do something super easy and give something super hard\n",
        "#that you never saw for homework. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX04U5Jmw2QH"
      },
      "source": [
        "-----\n",
        "\n",
        "## Use Custom Lp Space class in modeling\n",
        "\n",
        "Let's create a function that returns complied keras models so that we can run an experiment in order to compare the modeling results from using a keras L2 and our custom L2 regularizer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t36Cy3p2w2QI"
      },
      "source": [
        "-----\n",
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CHCfGNzw2QI"
      },
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load in and normalize image data set\n",
        "    \"\"\"\n",
        "    \n",
        "    # load in our dataset \n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "    # normalize pixel values between 0 and 1 \n",
        "    max_pixel_value = X_train.max()\n",
        "    X_train, X_test = X_train /max_pixel_value , X_test / max_pixel_value\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_zVLxxLw2QI",
        "outputId": "5c5763da-a098-40dc-a092-a15c39f71453"
      },
      "source": [
        "X_train, y_train, X_test, y_test = load_data()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Ubrhbtw2QI",
        "outputId": "9ba6e83f-57d0-41dd-9d95-5a0057c54aa3"
      },
      "source": [
        "# this is equal to the number of nodes in the output layer\n",
        "N_labels = len(np.unique(y_train))\n",
        "N_labels"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXwiYyk3w2QI"
      },
      "source": [
        "def create_model(reg=None):\n",
        "    # instantiate Sequential class\n",
        "    model = Sequential([\n",
        "\n",
        "    # flatten images \n",
        "        Flatten(input_shape=(28,28)), # images will be flattend out to 784 dims row vectors \n",
        "\n",
        "    # hidden layer 1\n",
        "        Dense(500, kernel_regularizer=reg),\n",
        "\n",
        "    # act func 1\n",
        "        ReLU(negative_slope=0.01),\n",
        "\n",
        "    # output layer \n",
        "        Dense(10, activation =\"softmax\")   \n",
        "    ])\n",
        "\n",
        "    # compile model \n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", \n",
        "                  optimizer= \"adam\", \n",
        "                  metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvCyAWFTw2QJ"
      },
      "source": [
        "----\n",
        "\n",
        "## Experiment 1: Calculate vector lengths using Keras and Custom Regularizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH0xcYKPw2QJ"
      },
      "source": [
        "\n",
        "\n",
        "If you read the [**Keras Documentation**](https://keras.io/api/layers/regularizers/) for their implementation of L2 regularization you'll see this:\n",
        "\n",
        "    The L2 regularization penalty is computed as: loss = l2 * reduce_sum(square(x))\n",
        "    \n",
        "Notice that Keras isn't taking the square root for L2 where as we are take the square root. \n",
        "\n",
        "Why isn't Keras taking the square root? I don't know but we will stay true to the math and take the square root for our implementation of L2 regularization. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFyqguMDw2QJ"
      },
      "source": [
        "# Did see this stuff in class or in the module notes. I know you want me to learn but this is just like univerisity class teach you how to do something super easy and give something super hard\n",
        "#that you never saw for homework. \n",
        "custom_l2 = Lp_distance_metric_general_formula(p=2, reg_strength = 1.0)\n",
        "# the length of the W vector from above as calcualted by the formula that we derived \n",
        "#custom_l2(W).numpy()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "lHprAoztw2QK",
        "outputId": "020468ca-ac85-4e04-d8ca-746a1f972507"
      },
      "source": [
        "# Did see this stuff in class or in the module notes. I know you want me to learn but this is just like univerisity class teach you how to do something super easy and give something super hard\n",
        "#that you never saw for homework. \n",
        "from keras.regularizers import L2\n",
        "keras_l2 = L2(l2=1.0)\n",
        "# again Keras doesn't take the square root\n",
        "#keras_l2(W).numpy()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a2dbe8093eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkeras_l2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# again Keras doesn't take the square root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkeras_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'W' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "uexQwey8w2QK",
        "outputId": "709ffc53-3ca0-453e-e098-a6eedecb7db2"
      },
      "source": [
        "# Did see this stuff in class or in the module notes. I know you want me to learn but this is just like univerisity class teach you how to do something super easy and give something super hard\n",
        "#that you never saw for homework. \n",
        "# but if we take the square root, then we get the same answer that our custom regularizer class gives us \n",
        "#np.sqrt(keras_l2(W).numpy())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a4f1b28ef0b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# but if we take the square root, then we get the same answer that our custom regularizer class gives us\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'W' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxewN-DVw2QL"
      },
      "source": [
        "----\n",
        "## Experiment 2: Compare Keras L2 and Custom L2 regularization in a model\n",
        "\n",
        "\n",
        "### Use Keras Built-in L2 regularizer \n",
        "\n",
        "Make sure to use the same regularization strength in the Keras pre-built L2 regularizer and in our custom L2 regularizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra3-VXCnw2QM",
        "outputId": "bd015537-c277-4e8b-b1a1-ad3e44e3f11a"
      },
      "source": [
        "model_keras_l2 = create_model(reg=L2(l2=0.01))\n",
        "keras_l2_results = model_keras_l2.fit(X_train, y_train,\n",
        "                                      epochs=1,\n",
        "                                      validation_data=(X_test, y_test), \n",
        "                                      workers=10) # check! You might not be able to use 10 processors on your machine "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 24s 6ms/step - loss: 1.6312 - accuracy: 0.7605 - val_loss: 0.7735 - val_accuracy: 0.7633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krL5lfe9w2QN"
      },
      "source": [
        "### Use Custom Regularization Class \n",
        "\n",
        "Make sure to use the same regularization strength in the Keras pre-built L2 regularizer and in our custom L2 regularizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N08Lj1h5w2QN"
      },
      "source": [
        "# set p= 2 so that we are using the same L2 regularizer as above\n",
        "lp = Lp_distance_metric_general_formula(p=2, reg_strength = 0.01)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "m1UTBjtgw2QN",
        "outputId": "7ef71605-81a8-4a83-d8a4-18ae98afc63c"
      },
      "source": [
        "model_custom_l2 = create_model(reg=lp)\n",
        "custom_l2_results = model_custom_l2.fit(X_train, y_train,\n",
        "                                      epochs=1,\n",
        "                                      validation_data=(X_test, y_test), \n",
        "                                      workers=10) # check! You might not be able to use 10 processors on your machine "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3942a03618ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                       workers=10) # check! You might not be able to use 10 processors on your machine \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:830 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:813 run_step  *\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:771 train_step  *\n        loss = self.compiled_loss(\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1369 losses\n        loss_tensor = regularizer()\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1445 _tag_callable\n        loss = loss()\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:2391 _loss_for_variable\n        regularization = regularizer(v)\n    <ipython-input-4-07bd3013b58f>:79 __call__\n        self.calc_squared_vector_comps()\n    <ipython-input-4-07bd3013b58f>:42 calc_squared_vector_comps\n        raise NotImplementedError()\n\n    NotImplementedError: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RsUb7PKw2QO"
      },
      "source": [
        "----\n",
        "### Compare Model Results\n",
        "\n",
        "Let's compare the modeling results between the two models. \n",
        "\n",
        "Whatever the specific test accuracies are, it should be the case that using our custom class leads to slightly better results than using the Keras version of $L_2$ regularization.\n",
        "\n",
        "There a small difference between test accuracies, possible due to the random sampling for the initial weight values. But also possibly from the observation that we made above - Keras isn't taking the square root. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0417yWfw2QP"
      },
      "source": [
        "_, keras_acc = model_keras_l2.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RI4n6xpw2QP"
      },
      "source": [
        "_, custom_acc = model_custom_l2.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gK5rLCyw2QP"
      },
      "source": [
        "\n",
        "### Conclusion \n",
        "\n",
        "In this experiment, we have bridged the gap between the theory of **$L_p$ Space Regularization** and the practice of using the general formula as a starting point to derive and build our very own regularizers. \n",
        "\n",
        "This experiment (and the Perceptron that we build from scratch in Sprint 2 Module 1) are examples of **mathematical algorithms** - an algorithm in mathematics is a procedure, a description of a set of steps that can be used to solve a mathematical computation.\n",
        "\n",
        "It's one thing to use open-source pre-built mathematical algorithms like Keras regularizers and Sklearn ML models but it's quite another thing to build them from scratch. \n",
        "\n",
        "In industry, you'll need to know when you should build something from scratch and when to use an open source solution. The rule of thumb is don't build it if someone else already has because maintaining the code and fixing bugs is something that other developers (ie. the open-source community) can spend their time on and you don't have to. But if an open source solution doesn't exist for a solution that you need, then you'll need to build it or try an alternative solution.\n",
        "\n",
        "It's a good thing you're taking the time to practice building some mathematical algorithms from scratch. It's a very valuable skill to have. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9j1DAqIw2QP"
      },
      "source": [
        "-----\n",
        "# GridSearch Experiments \n",
        "\n",
        "The next set of experiments will all involve gridsearching regularization parameter values. \n",
        "\n",
        "The rest of the notebook will actually require very little coding on your part. Instead, the focus is for you to run those gridsearches and answer the questions at the end of each experiment. Those questions are designed to help you capture the insights that there are to learn from each of the experiments. \n",
        "\n",
        "All of the following experiments are designed to help you better understand the relationship between the various regularization techniques and how they affect model performance. \n",
        "\n",
        "\n",
        "### Build Model\n",
        "\n",
        "Let's build out the model that we'll be using all throughout our experiments. \n",
        "\n",
        "Remember that **the whole point of regularization is to prevent overfitting.**\n",
        "\n",
        "\n",
        "![](https://hackernoon.com/hn-images/1*vuZxFMi5fODz2OEcpG-S1g.png)\n",
        "\n",
        "Overfitting happens when are model's are too complex, so in order to see a benefit from the use of regularization techniques we need to build a relatively complex model. \n",
        "\n",
        "Having said that, you might not have the computational resource to be able to train a complex model in a reasonable amount of time. So if this describes you, then you might want to consider using `build_simple_model`. Otherwise, I recommend that you use `build_complex_model`. \n",
        "\n",
        "This notebook will be using  `build_complex_model` to run our experiments. \n",
        "\n",
        "**NOTE:** Whichever function you end up using to build a model, take time to read through the code and make sure you understand what is happening. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ikpSFwAw2QQ"
      },
      "source": [
        "def build_complex_model(Lp_reg=None, reg_penality=None, dropout_prob=0.0, maxnorm_wc=None):\n",
        "    \"\"\"\n",
        "    Build and return a regularized 3 hidden layer FCFF model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Lp_reg: None or object\n",
        "        If object, Lp_reg is either l1 or l2 regularization \n",
        "        If None, that means that l1 or l2 regularization will not be used.\n",
        "     \n",
        "    reg_penality: None or float\n",
        "        If float, reg_penality is a value typically between 1.0 and 0.0001\n",
        "        This is the regularization strength for l1 or l2 \n",
        "        \n",
        "        \n",
        "    dropout_prob: float\n",
        "        This is the probability that dropout regularization will exclude a node from a training iteration. \n",
        "        If this value is 0.0, that means that dropout will not be used. \n",
        "        \n",
        "    maxnorm_wc: None or float\n",
        "        If float, maxnorm_wc is the weight constraint that is used for Max Norm regularization\n",
        "        If None, that means that Max Norm regularization will not be used.\n",
        "        \n",
        "        \n",
        "    Return\n",
        "    ------\n",
        "    model: complied Keras model\n",
        "    \"\"\"\n",
        "    \n",
        "    # if reg_type is not None, then pass in the penality strength to whatever form of Lp space regularization this is \n",
        "    if Lp_reg is not None:\n",
        "        Lp_regularizer = Lp_reg(reg_penality)\n",
        "    else:\n",
        "        Lp_regularizer = None\n",
        "                \n",
        "    if maxnorm_wc is not None:\n",
        "        wc = MaxNorm(max_value=maxnorm_wc)\n",
        "    else:\n",
        "        wc = None\n",
        "\n",
        "\n",
        "    # instantiate Sequential class\n",
        "    model = Sequential([\n",
        "\n",
        "    # flatten images \n",
        "    Flatten(input_shape=(28,28)),\n",
        "\n",
        "    # hidden layer 1\n",
        "    Dense(500, kernel_regularizer=Lp_regularizer , kernel_constraint=wc), # remember that Keras refers to weight matrix as a kernel, i.e. weights = kernel\n",
        "    # act func 1\n",
        "    ReLU(negative_slope=0.01),\n",
        "    Dropout(dropout_prob),\n",
        "\n",
        "    # hidden layer 2\n",
        "    Dense(250, kernel_regularizer=Lp_regularizer, kernel_constraint=wc),\n",
        "    # act func 2\n",
        "    ReLU(negative_slope=0.01),\n",
        "    Dropout(dropout_prob),\n",
        "\n",
        "    # hidden layer 3\n",
        "    Dense(100, kernel_regularizer=Lp_regularizer, kernel_constraint=wc),\n",
        "    # act func 2\n",
        "    ReLU(negative_slope=0.01),\n",
        "    Dropout(dropout_prob),\n",
        "\n",
        "    # output layer   \n",
        "    Dense(N_labels, activation=\"softmax\")  \n",
        "\n",
        "    ])\n",
        "    # complie model \n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", \n",
        "                 optimizer=\"adam\", \n",
        "                 metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqBaN1rgw2QR"
      },
      "source": [
        "Again, only use `build_simple_model` instead of `build_complex_model` if you're working on a machine with very limited computational resource. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0vq2tVxw2QR"
      },
      "source": [
        "# def build_simple_model(Lp_reg=None, reg_penality=None, dropout_prob=0, maxnorm_wc=None):\n",
        "#     \"\"\"\n",
        "#     Build and return a regularized 1 hidden layer FCFF model \n",
        "    \n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     Lp_reg: None or object\n",
        "#         If object, Lp_reg is either l1 or l2 regularization \n",
        "#         If None, that means that l1 or l2 regularization will not be used.\n",
        "     \n",
        "#     reg_penality: None or float\n",
        "#         If float, reg_penality is a value typically between 1.0 and 0.0001\n",
        "#         This is the regularization strength for l1 or l2 \n",
        "        \n",
        "        \n",
        "#     dropout_prob: float\n",
        "#         This is the probability that dropout regularization will exclude a node from a training iteration. \n",
        "#         If this value is 0.0, that means that dropout will not be used. \n",
        "        \n",
        "#     maxnorm_wc: None or float\n",
        "#         If float, maxnorm_wc is the weight constraint that is used for Max Norm regularization\n",
        "#         If None, that means that Max Norm regularization will not be used.\n",
        "        \n",
        "        \n",
        "#     Return\n",
        "#     ------\n",
        "#     model: complied Keras model\n",
        "#     \"\"\"\n",
        "    \n",
        "#     if Lp_reg is not None:\n",
        "#         Lp_regularizer = Lp_reg(reg_penality)\n",
        "#     else:\n",
        "#         Lp_regularizer = None\n",
        "\n",
        "#     # instantiate Sequential class\n",
        "#     model = Sequential([\n",
        "\n",
        "#     # flatten images \n",
        "#     Flatten(input_shape=(28,28)),\n",
        "\n",
        "#     # hidden layer 1\n",
        "#     Dense(128,  kernel_regularizer=Lp_regularizer, kernel_constraint=maxnorm_wc), # remember that Keras refers to weight matrix as a kernel, i.e. weights = kernel\n",
        "#     # act func 1\n",
        "#     ReLU(negative_slope=0.01),\n",
        "#     Dropout(p_dropout),\n",
        "\n",
        "#     # output layer   \n",
        "#     Dense(N_labels, activation=\"softmax\")  \n",
        "\n",
        "#     ])\n",
        "#     # complie model \n",
        "#     model.compile(loss=\"sparse_categorical_crossentropy\", \n",
        "#                  optimizer=\"adam\", \n",
        "#                  metrics=[\"accuracy\"])\n",
        "    \n",
        "#     return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ6iX0iTw2QS"
      },
      "source": [
        "Since we'll be using Sklearn's GridserchCV class, we need to wrap our Keras models in `KerasClassifier`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FVJzllKw2QS"
      },
      "source": [
        "# remember to wrap KerasClassifier around build_model for sklearn's GridsearchCV compatibility \n",
        "model = KerasClassifier(build_fn = build_complex_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir___9Www2QS"
      },
      "source": [
        "-------\n",
        "\n",
        "# Experiment 1: Identify the relationship between model performance and L2 penalty strength\n",
        "\n",
        "![](https://www.researchgate.net/publication/334159821/figure/fig1/AS:776025558495234@1562030319993/Ridge-regression-variable-selection.png)\n",
        "\n",
        "We are going to run a gridsearch soley on the l2 regularization penalty value and see the effect this has on model performance. \n",
        "\n",
        "By running a gridseach on only a single hyperparameter (while using the same data and model) we can isolate the effect of that hyperparameter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP0Hwn5hw2QS"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # take note that Lp_reg penalty/strength values are in powers of 10 \n",
        "    \"reg_penality\": [1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001], \n",
        "    # Since we only want to test l2, provide l2 as the sole option \n",
        "    \"Lp_reg\": [l2],\n",
        "    # default is 1, in order to change it we must provide value here because we can't provide a parameter value for model.fit() directly when using gridsearch\n",
        "    # protip: consider chanign epochs to 1 if the gridsearche run-time are too long for you\n",
        "    \"epochs\": [1] \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIb8_s7vw2QT"
      },
      "source": [
        "start=time()\n",
        "# Create and run Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-3, \n",
        "                    verbose=1, \n",
        "                    cv=2)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "end=time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diyT0Efsw2QT"
      },
      "source": [
        "print(\"Gridsearch runtime {0:.3} mins\".format( (end-start)/60 ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5rqJlNpw2QU"
      },
      "source": [
        "# use the mean accuracy from the CV splits for determining best model score \n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "# move l2 penalty values outside of dictionary and into a list\n",
        "param_values = [dic[\"reg_penality\"] for dic in params]\n",
        "\n",
        "# plot accuracy vs l2_reg_penalty\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.grid()\n",
        "\n",
        "# this plot is using the std of the CV splits to plot error bars however those values are so small that they aren't visable\n",
        "plt.errorbar(param_values, means, yerr=stds, ecolor=\"orange\")\n",
        "plt.xscale(\"log\") # use a log scale for ease of reading, recall that l2_reg_penalty were in powers of 10 \n",
        "plt.title(\"L2 Regularization: Model Accuracy vs L2 Penalty Strength\")\n",
        "plt.ylabel(\"Validation Accuracy\", )\n",
        "plt.xlabel(\"L2 Penalty Strength usng a Log Scale\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOv0qQW-w2QU"
      },
      "source": [
        "### Observations\n",
        "\n",
        "Write down some observations. What do you notice from the plot?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "089b55b5a84d9c96c51fd341d9e6c74f",
          "grade": true,
          "grade_id": "cell-010212fc0915b976",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "6q6dNpMHw2QU"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7dC1dd5w2QU"
      },
      "source": [
        "\n",
        "## Compare Weights between Best and Worse Model \n",
        "\n",
        "Next, we are going to compare the hidden layer weights between the best and worse performing model while taking note of the respective l2 penalty strengths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67V_IJfHw2QU"
      },
      "source": [
        "# get the best l2 penalty term \n",
        "best_lr_penalty = grid_result.best_params_[\"reg_penality\"]\n",
        "\n",
        "# get the best trained model \n",
        "best_model = grid_result.best_estimator_.build_fn(Lp_reg=l2, reg_penality=best_lr_penalty)\n",
        "\n",
        "# get the weights from the best trained model \n",
        "best_weights = best_model.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ify-RC0w2QU"
      },
      "source": [
        "# train a model using the l2_reg_penalty value at scored the lowest \n",
        "worse_l2_reg_penalty = 1.0\n",
        "\n",
        "worse_model = build_complex_model(Lp_reg=l2, reg_penality=worse_l2_reg_penalty)\n",
        "\n",
        "# fit model \n",
        "worse_model.fit(X_train, y_train, epochs=1)\n",
        "\n",
        "# get weights from worse performing model \n",
        "worse_weights = worse_model.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpKBf24Ww2QV"
      },
      "source": [
        "-----\n",
        "## Understanding how Weights and Biases are stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjnXZF78w2QV"
      },
      "source": [
        "Let's take a minute to understand that`.get_weights()` returns a list with 8 elements (if you're using `build_complex_model`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2INMyTVLw2QV"
      },
      "source": [
        "len(best_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm_cpdXtw2QV"
      },
      "source": [
        "There are **weights matrices and bias vectors between each layer** and we have 5 layers. \n",
        "\n",
        "- Input\n",
        "- Hidden 1\n",
        "- Hidden 2\n",
        "- Hidden 3\n",
        "- Output \n",
        "\n",
        "So that means we should have 4 weight matrices, but we see 8. **There are also 4 weight vectors for the biases between each layer.** So that accounts for the 8. \n",
        "\n",
        "\n",
        "#### Index for Weight Matrices \n",
        "If you index for a weight matrix, you can see its shape and that they are indeed matrices. \n",
        "\n",
        "Notice how you can see the dims of the layers that the matrices are sandwiched between?\n",
        "\n",
        "Input layer has 784 dims and hidden layer 1 has 500 dims. Given this understanding, the numbers you see in the shapes should make sense. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJdayppcw2QW"
      },
      "source": [
        "# bewteen input and 1st hidden layer\n",
        "best_weights[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSwWSBPlw2QW"
      },
      "source": [
        "# bewteen 1st and 2nd hidden layer\n",
        "best_weights[2].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6Bs3RTrw2QW"
      },
      "source": [
        "# bewteen 2st and 3nd hidden layer\n",
        "best_weights[4].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi768pPew2QW"
      },
      "source": [
        "# bewteen 3rd hidden layer and output layer\n",
        "best_weights[6].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwDi5Iurw2QX"
      },
      "source": [
        "#### Index for the bias vectors\n",
        "\n",
        "The shapes of the bias vectors should exactly match up the dims/nodes of each layer (excluding the input layer). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYzIDA22w2QX"
      },
      "source": [
        "# for hidden layer 1 \n",
        "best_weights[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w70YiRJDw2QX"
      },
      "source": [
        "# for hidden layer 2 \n",
        "best_weights[3].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze6UO0diw2QX"
      },
      "source": [
        "# for hidden layer 3\n",
        "best_weights[5].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixL74WJyw2QX"
      },
      "source": [
        "# for output layer\n",
        "best_weights[7].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Ea6Zaww2QX"
      },
      "source": [
        "-----\n",
        "\n",
        "### Back to our Analysis of L2 space regularization (also known as Ridge)\n",
        "\n",
        "Let's do a comparison of the first weight matrix (between the input and 1st hidden layer) for the best and worse performing model as well as with the initial weight values that are randomly sampled from the GlorotUniform distribution.\n",
        "\n",
        "[**Check out the Keras docs for the Dense layer**](https://keras.io/api/layers/core_layers/dense/), you'll see that GlorotUniform is the default weight initializer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAh-7m-Zw2QY"
      },
      "source": [
        "Before we compare weights, let's take note of the following. \n",
        "\n",
        "Both `best_weights[0]` or `worse_weights[0]` are matrices with shape `(784, 500)`. \n",
        "\n",
        "If we flatten them, then we get `784 * 500 = 392000` weights. What does this mean exactly?\n",
        "\n",
        "Remember that we are working with the Fully Connected Forward Feeding model which looks something like this. \n",
        "\n",
        "![](https://pyimagesearch.com/wp-content/uploads/2016/08/simple_neural_network_header.jpg)\n",
        "\n",
        "The important thing to notice is that in `Fully Connected` models, each component in the input vectors gets passed to all the nodes in next layer. This means for our input vector with 784 dims, there are 784 **$w_1$** weights, each with a slightly different value, one for each of the 784 components in the input vector. \n",
        "\n",
        "So to keep our analysis simple, we are going to just analyze one column of the weight matrix. You could index for whatever column you want, but this notebook will assume that you have indexed for the weights in column one. This means that we will compare the affect that regularization had on all of the 784 **$w_1$** weights that exist in 500 dimensional space of hidden layer 1. \n",
        "\n",
        "In other words, by looking at just one column, we are narrowing our analysis to the distribution of values for w_1 oppose to mixing together the distributions of all the weights w_1, w_2, ..., w_500. This is done for simplicity of analysis, the goal being to empirically observe the affect of regularization on the weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a3f75126a80a74dd71fb0a9ab2b8b89f",
          "grade": false,
          "grade_id": "cell-7882876b8973bc7b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "oy889Skaw2QY"
      },
      "source": [
        "# index for the 1st column in the 1st hidden layer weights in best_weights and save to best_hidden_weights\n",
        "\n",
        "# index for the 1st column in the 1st hidden layer weights in worse_weights and save to worse_hidden_weights\n",
        "\n",
        "# Keras models randomly samples from the GlorotUniform distribution for the initial values of model weights \n",
        "# instantiate GlorotUniform and sample 500 weights and save to initial_weight_values\n",
        "# hint: use shape=(500,1)\n",
        "\n",
        "# lastly, flatten all of the above 3 variables so that you go from a matrix to a vector \n",
        "# otherwise, pandas will not like you and throw a fit \n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0alNRKqw2QZ"
      },
      "source": [
        "# move all weights to a dataframe for ease of analysis \n",
        "cols = [\"best_hidden_weights\", \"worse_hidden_weights\", \"initial_weight_values\"]\n",
        "data = [best_hidden_weights, worse_hidden_weights, initial_weight_values]\n",
        "df = pd.DataFrame(data=data).T\n",
        "df.columns = cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ASphEiPw2QZ"
      },
      "source": [
        "# check out the statistics for each weight column \n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sprGaMEw2Qa"
      },
      "source": [
        "# plot the distributions for each weight column \n",
        "df.hist(figsize=(20,12));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRGxAt1nw2Qb"
      },
      "source": [
        "## Observations \n",
        "\n",
        "Take a look at the statistical table and the plots. Then answer the following questions. \n",
        "\n",
        "**How do the hidden layer weights from the best performing model compare to the initial weight values?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5eacb66f89b216ae3b4bb3c4b7bd6d38",
          "grade": true,
          "grade_id": "cell-6add7cc400c4c716",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bbnykVUOw2Qb"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9XzxiGgw2Qc"
      },
      "source": [
        "**What was the effect of using a small l2 penalty value?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "92ac1689b72d727ab7f4d261c5e76daa",
          "grade": true,
          "grade_id": "cell-5b4f11bba2d49639",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7vkWhVV8w2Qc"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLbF2xipw2Qc"
      },
      "source": [
        "**What was the effect of using a large l2 penalty value?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3048a6d2805f61d6fb58372c42c3ac54",
          "grade": true,
          "grade_id": "cell-0a30b62e5e119555",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "2f5IY1xlw2Qc"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBg7ybnZw2Qc"
      },
      "source": [
        "**Given what you know about L2 regularization, are you surprised by these results?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d8a2034f67badfe53f601873f7026dc0",
          "grade": true,
          "grade_id": "cell-c04d067161064011",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8o4N1QvOw2Qc"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1HhUOsSw2Qc"
      },
      "source": [
        "----\n",
        "\n",
        "# Experiment 2: Identify the relationship between model performance and Max Norm Weight Constraint\n",
        "\n",
        "![](https://qph.fs.quoracdn.net/main-qimg-9d0dbf8074761b541ba80543ddfc9f73.webp)\n",
        "\n",
        "Recall from lecture that the **Norm** of a vector is just another word for the **length** of a vector.\n",
        "\n",
        "MaxNorm weight constraint puts a limit on the length of a weight vector.\n",
        "\n",
        "$$ \\text{Max_value_of_norm} >= {\\displaystyle \\left\\|x\\right\\|_{p}=\\left(|x_{1}|^{p}+|x_{2}|^{p}+\\dotsb +|x_{n}|^{p}\\right)^{1/p}.}$$  \n",
        "\n",
        "The effect that Lp regularization and Max Norm Weight Constraint have on the weights should be the same, but they go about it in different ways. \n",
        "\n",
        "Lp regularization (l1/Lasso and l2/Ridge) shrink the value of the weights. Where as Max Norm Weight Constraint puts a limit on how big the weight vector can be which, in effect, keeps the individual weight values small enough to keep the norm below that limit. \n",
        "\n",
        "In this experiment, we are going to run another gridseach but instead of using Lp space regularization as we did in the previous experiment, we are going to use MaxNorm and see what kind of effect that this type of regularization has on model performance and the learned weights. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzxt_uRdw2Qd"
      },
      "source": [
        "Since we already built our model, we just need to update the `hyper_parameters` dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHmqcd59w2Qd"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    \n",
        "    \"maxnorm_wc\": np.linspace(0.5, 10.0, num=20), \n",
        "    # default is 1, in order to change it we must provide value here because we can't provide a parameter value for model.fit() directly when using gridsearch\n",
        "    # protip: consider changing epochs to 1 if the gridsearche run-time are too long for you    \n",
        "    \"epochs\": [1] \n",
        "}\n",
        "\n",
        "hyper_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvCBMN9Gw2Qd"
      },
      "source": [
        "start=time()\n",
        "# Create and run Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=2)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "end=time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFiq_6Hfw2Qd"
      },
      "source": [
        "print(\"Gridsearch runtime {0:.3} mins\".format( (end-start)/60 ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAbETxZZw2Qd"
      },
      "source": [
        "# use the mean accuracy from the CV splits for determining best model score \n",
        "means = grid.cv_results_['mean_test_score']\n",
        "stds = grid.cv_results_['std_test_score']\n",
        "params = grid.cv_results_['params']\n",
        "\n",
        "# move l2 penalty values outside of dictionary and into a list\n",
        "param_values = [dic[\"maxnorm_wc\"] for dic in params]\n",
        "\n",
        "# plot accuracy vs l2_reg_penalty\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.grid()\n",
        "plt.errorbar(param_values, means, yerr=stds, ecolor=\"orange\")\n",
        "plt.title(\"L1 Regularization: Model Accuracy vs L1 Penalty Strength\")\n",
        "plt.ylabel(\"Validation Accuracy\", )\n",
        "plt.xlabel(\"Max Norm for Weight Vector \");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cfe7bd54a7a14ba7ee63c88d6d1828b4",
          "grade": false,
          "grade_id": "cell-f67372e0b9b30614",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5SuF89egw2Qe"
      },
      "source": [
        "# get the best l2 penalty term from grid and save to best_max_norm_val\n",
        "\n",
        "# get the best trained model from grid and save to best_model\n",
        "\n",
        "# get the weights from the best trained model and save to best_weights\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ujqDT1bw2Qe"
      },
      "source": [
        "best_max_norm_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB2DEn2Ow2Qe"
      },
      "source": [
        "# we see that the norm of our weights are indeed below the maximum allowed value \n",
        "np.linalg.norm(best_weights[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "67f65bd636e3b3b3bc7d20c02ba6b666",
          "grade": false,
          "grade_id": "cell-e752c1a8c853985d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "jg1VsJ6kw2Qe"
      },
      "source": [
        "# train a model using the max_norm_val value that scored the lowest \n",
        "\n",
        "# build a model using build_complex_model and worse_max_norm_val and save it to worse_model\n",
        "\n",
        "# fit model \n",
        "\n",
        "# get weights from worse performing model \n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a38d5d4db707124b31a662fb4743b049",
          "grade": false,
          "grade_id": "cell-5c1aa4543e68487d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "phfIipWRw2Qe"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMGzBB9Cw2Qe"
      },
      "source": [
        "# move all weights to a dataframe for ease of analysis \n",
        "cols = [\"best_hidden_weights\", \"worse_hidden_weights\", \"initial_weight_values\"]\n",
        "data = [best_hidden_weights, worse_hidden_weights, initial_weight_values]\n",
        "df_maxnorm= pd.DataFrame(data=data).T\n",
        "df_maxnorm.columns = cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh-9nKouw2Qe"
      },
      "source": [
        "df_maxnorm.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb_L9ux4w2Qf"
      },
      "source": [
        "# plot the distributions for each weight column \n",
        "df_maxnorm.hist(figsize=(20,12));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaEjvvE6w2Qf"
      },
      "source": [
        "## Observations \n",
        "\n",
        "Take a look at the statistical table and the plots. Then answer the following questions. \n",
        "\n",
        "**How do the hidden layer weights from the best performing model compare to the initial weight values?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f43fe1110cdcea8d1e4b432fe78d4e49",
          "grade": true,
          "grade_id": "cell-40a44d19694941b8",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "MCP06NfOw2Qf"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHZ0lSwZw2Qf"
      },
      "source": [
        "**What was the effect of using the weight constraint value in MaxNorm in the best performing model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b1c59c58a5abdbc0b509983821198dba",
          "grade": true,
          "grade_id": "cell-4f9e1e134124e512",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "6q9nwNnFw2Qf"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jihxoemw2Qf"
      },
      "source": [
        "**What was the effect of using the weight constraint value in MaxNorm in the worse performing model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0062b4ddfad487c39633c37f4710b752",
          "grade": true,
          "grade_id": "cell-4c289ce70c34048a",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "h_YP0Ekbw2Qf"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKl4pa-hw2Qf"
      },
      "source": [
        "**Given what you know about MaxNorm regularization, are you surprised by these results?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3c36931d3532a8cbcb4ea0c956378728",
          "grade": true,
          "grade_id": "cell-77366a912217da5d",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "WBbuFu6xw2Qf"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nejst15iw2Qg"
      },
      "source": [
        "-----\n",
        "# Experiment 3: Identify the relationship between model performance and Dropout\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/981/1*EinUlWw1n8vbcLyT0zx4gw.png)\n",
        "\n",
        "In the 3rd and final experiment, we will use gridsearch to see how model performance is affect by varying the value of the the dropout probability. \n",
        "\n",
        "Recall from lecture that dropout tends to perform best when used with weight constraint. Since this is the case, we will gridsearch both dropout probability and the weight constraint for MaxNorm. \n",
        "\n",
        "If interested, feel free to read through the original publication on [**Drop Out**](https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf). \n",
        "\n",
        "**Key Take aways:** \n",
        "\n",
        "1. During training, dropout will probabilistically \"turn off\" some neurons in the layer that dropout is implemented in. \n",
        "2. During inference (ie. making predictions on the test set) all neurons are used (i.e. no dropout is applied).\n",
        "3. Dropout works best when used with MaxNorm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us-unQTLw2Qg"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # for the sake of runtime, let's vary maxnorm_wc between 0.5 and 5.0\n",
        "    \"maxnorm_wc\": np.linspace(0.5, 5, num=10),\n",
        "    # take note that l1_reg_penalty values are in powers of 10 \n",
        "    \"dropout_prob\": np.linspace(0.0, 0.6, num=7), \n",
        "    \"epochs\": [1] # default is 1, in order to change it we must provide value here because we can provide a parameter value for model.fit() directly when using gridsearch\n",
        "}\n",
        "\n",
        "hyper_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cVSfRfbw2Qg"
      },
      "source": [
        "start=time()\n",
        "# Create and run Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "end=time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlpxgQpKw2Qg"
      },
      "source": [
        "print(\"Gridsearch runtime {0:.3} mins\".format( (end-start)/60 ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb9KBVi7w2Qg"
      },
      "source": [
        "# use the mean accuracy from the CV splits for determining best model score \n",
        "means = grid.cv_results_['mean_test_score']\n",
        "stds = grid.cv_results_['std_test_score']\n",
        "params = grid.cv_results_['params']\n",
        "\n",
        "# move l2 penalty values outside of dictionary and into a list\n",
        "param_values = [dic[\"dropout_prob\"] for dic in params]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b0YLinzw2Qh"
      },
      "source": [
        "Since there are 2 indepdent variables this time around (dropout_prob and maxnorm_wc) which affect the validation accuracy, it's best to use a different plot. A heat map will work. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC3dwsIAw2Qh"
      },
      "source": [
        "dropout_prob_list = [  dic[\"dropout_prob\"]  for dic in params]\n",
        "maxnorm_wc_list = [  dic[\"maxnorm_wc\"]  for dic in params]\n",
        "data = [means, dropout_prob_list, maxnorm_wc_list ]\n",
        "\n",
        "cols = [\"val_acc\", \"dropout_prob\", \"maxnorm_wc\"]\n",
        "df_exp3 =pd.DataFrame(data=data).T\n",
        "df_exp3.columns = cols\n",
        "df_exp3.dropout_prob = df_exp3.dropout_prob.round(2)\n",
        "\n",
        "# pivot dataframe in preperation for heat map\n",
        "df_exp3 = df_exp3.pivot(\"maxnorm_wc\", \"dropout_prob\", \"val_acc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWdrFGoew2Qh"
      },
      "source": [
        "# Draw a heatmap with the val_acc values in each cell\n",
        "f, ax = plt.subplots(figsize=(18, 8))\n",
        "sns.heatmap(df_exp3, annot=True,  linewidths=.5, ax=ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oDrBJd3w2Qh"
      },
      "source": [
        "### Observations \n",
        "\n",
        "We can see the dropout probabilities in the horizontal axis and the maxnorm weight constraint values in the virtical axis. The values in the cells are the validation accuracy that corresponds to a pair of regularization values.\n",
        "\n",
        "Take a look at the heat map and answer the following questions. Note that depending on which model you used (the simple or complex one) your answers might be different from that of others. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ugCTJlsw2Qh"
      },
      "source": [
        "**What range of dropout probability values tend to produce the highest validation accuracy?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0f0013d4e07104a03b4d51664a308f53",
          "grade": true,
          "grade_id": "cell-4e0cb7a9240b1531",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "CZvlyL4Ow2Qi"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_4gm8aOw2Qi"
      },
      "source": [
        "**What range of maxnorm weight constraints tend to produce the highest validation accuracy?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9fd88f0bb870a910b925d60b38d17694",
          "grade": true,
          "grade_id": "cell-99539755d7d328f7",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5t3e6qc7w2Qi"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNDctfNlw2Qi"
      },
      "source": [
        "**When taken together, what pair of dropout probability and maxnorm weight constraints tend to produce the highest validation accuracy?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fee1e09ed8f6d354bd7b6e2986c2b811",
          "grade": true,
          "grade_id": "cell-5e19a56b4a2d975d",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "C8LpZjUIw2Qi"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD0_k4a_w2Qi"
      },
      "source": [
        "**Do you think that using dropout was helpful in increasing model performance?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "de9c1bcff3c5eb6266cc80632d0956f0",
          "grade": true,
          "grade_id": "cell-d2a2f7b284c801dc",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "UWGW2pMtw2Qi"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVB4I8Byw2Qi"
      },
      "source": [
        "-----\n",
        "# Stretch Goals for $L_p$ Space section\n",
        "\n",
        "Here are some ideas that you can explore using the custom distance metric class that you have built. Though if you think of something else, go for it!\n",
        "\n",
        "- Run a similar experiment but instead of comparing L2 between Keras and our custom class, compare L1 (Lasso) \n",
        "- Run a similar experiment but instead of comparing L2 between Keras and our custom class, compare L1_L2 (Elastic Net) \n",
        "- Run a gridsearch across several different Lp distance metrics and strengths and see which Lp distance leads to the best performing model\n",
        "    - Consider selecting a p range between **[1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, ..., 10]** step size of 0.5\n",
        "    - Consider selecting a p range between **[1, 10, 100, 1000, 10000]** step size in powers of 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6hhWuKcw2Qj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjG_G_Rsw2Qj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1eWQDd_w2Qj"
      },
      "source": [
        "_____\n",
        "\n",
        "### Experiment 4: Train, Save, and Load a Keras model\n",
        "\n",
        "Let's get some practice with how to save and load trained Keras models \n",
        "\n",
        "For this experiment, review the section on Saving and Loading models from the guided project in order to help you to: \n",
        "\n",
        "- Build a model of your choosing\n",
        "- Gridsearch the model with a method of your choosing\n",
        "- Save the trained model to file\n",
        "- Load the trained model from file\n",
        "- Just as we did in the Guided Project, evalute the loaded model using a test set and make sure the results of the loaded model match that of the saved model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWPu4XKPw2Qj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElKEqzq-w2Qj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbJJh2MCw2Qj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}