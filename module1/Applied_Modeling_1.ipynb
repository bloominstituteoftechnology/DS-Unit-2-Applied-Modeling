{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Applied_Modeling_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewwhite5/DS-Unit-2-Applied-Modeling/blob/master/module1/Applied_Modeling_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLHMFpIo8QfO",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 3, Module 1*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3b4ePYmnR2Mp"
      },
      "source": [
        "Lambda School Data Science, Unit 2: Predictive Modeling\n",
        "\n",
        "# Applied Modeling, Module 1\n",
        "\n",
        "- Use classification metric: ROC AUC\n",
        "- Visualize the ROC curve by plotting true positive rate vs false positive rate at varying thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y84ku8F8-18",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "\n",
        "You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cr5P-ilWxa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "# If you're in Colab...\n",
        "if in_colab:\n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Applied-Modeling.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Install required python packages\n",
        "    !pip install -r requirements.txt\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6un6H1mlQA8",
        "colab_type": "text"
      },
      "source": [
        "## Process for Data Science\n",
        "\n",
        "#### Renee Teate, [Becoming a Data Scientist, PyData DC 2016 Talk](https://www.becomingadatascientist.com/2016/10/11/pydata-dc-2016-talk/)\n",
        "\n",
        "![](https://image.slidesharecdn.com/becomingadatascientistadvice-pydatadc-shared-161012184823/95/becoming-a-data-scientist-advice-from-my-podcast-guests-55-638.jpg?cb=1476298295)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_jSa6inlSXD",
        "colab_type": "text"
      },
      "source": [
        "#### _This diagram is general and high-level. How do we apply it when doing predictive modeling with labeled, tabular data?_\n",
        "\n",
        "Business Question ‚û° Data Question = steps 1-3 below\n",
        "\n",
        "Data Question ‚û° Data Answer = steps 4-6 below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XjSvr9qlU5b",
        "colab_type": "text"
      },
      "source": [
        "## Process for Supervised Learning\n",
        "\n",
        "#### Francois Chollet, [Deep Learning with Python](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/README.md), Chapter 4: Fundamentals of machine learning, \"A universal workflow of machine learning\"\n",
        " \n",
        "> **1. Define the problem at hand and the data on which you‚Äôll train.** Collect this data, or annotate it with labels if need be.\n",
        "\n",
        "> **2. Choose how you‚Äôll measure success on your problem.** Which metrics will you monitor on your validation data?\n",
        "\n",
        "> **3. Determine your evaluation protocol:** hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
        "\n",
        "> **4. Develop a first model that does better than a basic baseline:** a model with statistical power.\n",
        "\n",
        "> **5. Develop a model that overfits.** The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
        "\n",
        "> **6. Regularize your model and tune its hyperparameters, based on performance on the validation data.** Repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. \n",
        "\n",
        "> **Iterate on feature engineering: add new features, or remove features that don‚Äôt seem to be informative.** Once you‚Äôve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU6k27uMlYKm",
        "colab_type": "text"
      },
      "source": [
        "## 1. Define the problem at hand and the data on which you'll train\n",
        "\n",
        "This isn't easy! You have to define your target, join tables, and avoid leakage. This opinionated blog post explains:\n",
        "\n",
        "#### [Data Science Is Not Taught At Universities - And Here Is Why](https://www.linkedin.com/pulse/data-science-taught-universities-here-why-maciej-wasiak/)\n",
        "\n",
        "> The tables they use in machine learning research already have the target information clearly defined. Here comes the famous IRIS dataset, then the Wisconsin Breast Cancer, there is even Credit Risk or Telco Churn data and they all have the **Target** column there ...\n",
        "\n",
        "> The problem is that in real life the **Target** flag is NEVER there.\n",
        "\n",
        "> For churn modelling you may have many churn types on the system and need to distil the few that need modelling. And hey - when a subscriber moves from Postpaid contract to Prepaid ‚Äì is this a churn or not? (‚ÄòYes‚Äô ‚Äì says the Postpaid Base Manager, ‚ÄòNo‚Äô says the CEO ). You have to make the call ...\n",
        "\n",
        "> Your source will be a database with tens or hundreds of **tables**, millions of records, usually after 3 painful migrations with gaps in history, columns without descriptions ...\n",
        "\n",
        "> Flooded by **leaks from the future**, ...a dozen of other traps ... And you need to disarm all of them, because even one left behind may result in a completely useless model. \n",
        "\n",
        "> These are the skills employers are looking for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHo4K2iAldXW",
        "colab_type": "text"
      },
      "source": [
        "## Regression or Classification?\n",
        "\n",
        "#### You can convert problems from regression to classification\n",
        "\n",
        "1. UCI, [Adult Census Income dataset](https://archive.ics.uci.edu/ml/datasets/adult)\n",
        "\n",
        "2. DS5 student Han Lee, [Bitcoin Price Prediction app](https://dry-shore-97069.herokuapp.com/about):\n",
        "\n",
        "> We also cared a lot more about the direction of returns instead of magnitude of returns. A trade placed based on the prediction that the price to go up tomorrow will be fine if the magnitude is off but will be unprofitable if the direction is wrong. ... Yesterday's return is unsurprising a great predictor for today's return, but has a poor directional accuracy.\n",
        "\n",
        "#### You can convert problems from classification to regression\n",
        "\n",
        "Brandon Rohrer, [What questions can machine learning answer](https://brohrer.github.io/five_questions_data_science_answers.html)\n",
        "\n",
        "> Sometimes questions that look like multi-value classification questions are actually better suited to regression. For instance, ‚ÄúWhich news story is the most interesting to this reader?‚Äù appears to ask for a category‚Äîa single item from the list of news stories. However, you can reformulate it to ‚ÄúHow interesting is each story on this list to this reader?‚Äù and give each article a numerical score. Then it is a simple thing to identify the highest-scoring article. Questions of this type often occur as rankings or comparisons.\n",
        "\n",
        "> ‚ÄúWhich van in my fleet needs servicing the most?‚Äù can be rephrased as ‚ÄúHow badly does each van in my fleet need servicing?‚Äù \n",
        "‚ÄúWhich 5% of my customers will leave my business for a competitor in the next year?‚Äù can be rephrased as ‚ÄúHow likely is each of my customers to leave my business for a competitor in the next year?‚Äù \n",
        "\n",
        "> Binary classification problems can also be reformulated as regression. (In fact, under the hood some algorithms reformulate every binary classification as regression.) This is especially helpful when an example can belong part A and part B, or have a chance of going either way. When an answer can be partly yes and no, probably on but possibly off, then regression can reflect that. Questions of this type often begin ‚ÄúHow likely‚Ä¶‚Äù or ‚ÄúWhat fraction‚Ä¶‚Äù\n",
        "\n",
        "> How likely is this user to click on my ad? What fraction of pulls on this slot machine result in payout? How likely is this employee to be an insider security threat? What fraction of today‚Äôs flights will depart on time?\n",
        "\n",
        "We'll see examples of this, using predicted probabilities instead of discrete predictions, with Tanzania Waterpumps and Lending Club data.\n",
        "\n",
        "#### You can convert multi-class classification to binary classification\n",
        "\n",
        "By omitting or combining some classes. We'll also see examples of this, with Tanzania Waterpumps and Lending Club data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mqj_gSCzR2Mq"
      },
      "source": [
        "## Lending Club example üè¶\n",
        "\n",
        "### Background\n",
        "\n",
        "[According to Wikipedia,](https://en.wikipedia.org/wiki/Lending_Club)\n",
        "\n",
        "> Lending Club is the world's largest peer-to-peer lending platform. Lending Club enables borrowers to create unsecured personal loans between \\$1,000 and \\$40,000. The standard loan period is three years. Investors can search and browse the loan listings on Lending Club website and select loans that they want to invest in based on the information supplied about the borrower, amount of loan, loan grade, and loan purpose. Investors make money from interest. Lending Club makes money by charging borrowers an origination fee and investors a service fee.\n",
        "\n",
        "[Lending Club says,](https://www.lendingclub.com/) \"Our mission is to transform the banking system to make credit more affordable and investing more rewarding.\" You can view their [loan statistics and visualizations](https://www.lendingclub.com/info/demand-and-credit-profile.action).\n",
        "\n",
        "Lending Club's [Investor Education Center](https://www.lendingclub.com/investing/investor-education) can help you grow your domain expertise. The article about [Benefits of diversification](https://www.lendingclub.com/investing/investor-education/benefits-of-diversification) explains,\n",
        "\n",
        "> With the investment minimum of \\$1,000, you can get up to 40 Notes at \\$25 each.\n",
        "\n",
        "![](https://i.ibb.co/B37q8LB/www-lendingclub-com-browse-browse-action-1.png)\n",
        "\n",
        "### Data sources\n",
        "- [Current loans](https://www.lendingclub.com/browse/browse.action)\n",
        "- [Data Dictionary & Historical loans](https://www.lendingclub.com/info/download-data.action) (17 zip files, 450 MB total)\n",
        "\n",
        "### What questions could we ask with this data?\n",
        "1. Can we predict the interest rate that Lending Club will assign to a loan, to reverse engineer their formula. (Regression problem. Can only use info from before the interest rate was assigned)\n",
        "2. Can we predict whether a loan will be fully paid or charged off, to choose which loans to invest in. (Classification problem. Can only use info available at the time you choose loans, from loans that have been fully paid or charged off.)\n",
        "\n",
        "[Here's a Plotly Dash app for #1](https://rrherr-project2-example.herokuapp.com/).\n",
        "\n",
        "This notebook will work on #2.\n",
        "\n",
        "\n",
        "### Use a subset of Loan Status\n",
        "\n",
        "#### [Data-Driven Investment Strategies for Peer-to-Peer Lending: A Case Study for Teaching Data Science](https://www.liebertpub.com/doi/full/10.1089/big.2018.0092)\n",
        "\n",
        "> Current refers to a loan that is still being reimbursed in a timely manner. Late corresponds to a loan on which a payment is between 16 and 120 days overdue. If the payment is delayed by more than 121 days, the loan is considered to be in Default. If LendingClub has decided that the loan will not be paid off, then it is given the status of Charged-Off.\n",
        "\n",
        "> These dynamics imply that 5 months after the term of each loan has ended, every loan ends in one of two LendingClub states‚Äîfully paid or charged-off. We call these two statuses fully paid and defaulted, respectively, and we refer to a loan that has reached one of these statuses as expired.\n",
        "\n",
        "> **One way to simplify the problem is to consider only loans that have expired at the time of analysis.**\n",
        "\n",
        "> A significant portion (13.5%) of loans ended in Default status; depending on how much of the loan was paid back, these loans\n",
        "might have resulted in a significant loss to investors who had invested in them. The remainder was Fully Paid‚Äîthe borrower fully reimbursed the loan‚Äôs outstanding balance with interest, and the investor earned a positive return on his or her investment. Therefore, to avoid unsuccessful investments, our goal is to estimate which loans are more likely to default and which will yield low returns. \n",
        "\n",
        "### Use a subset of Loan Grade\n",
        "\n",
        "[Lending Club announced,](https://blog.lendingclub.com/q1-2019-platform-update) \n",
        "\n",
        "> We periodically adjust platform products to reflect changes in investor demand and other marketplace factors. As a result, this quarter we are retiring Grade E loans. As of May 7, 2019, we will no longer facilitate new Grade E loans except for certain previously qualified or approved loans; **effective July 1, 2019, no grade E loans will be available on the platform.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0X_ZtsxXR2Mt",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_columns = 200\n",
        "pd.options.display.max_rows = 200\n",
        "\n",
        "history_location = '../data/lending-club/lending-club-subset.csv'\n",
        "current_location = '../data/lending-club/primaryMarketNotes_browseNotes_1-RETAIL.csv'\n",
        "\n",
        "# Stratified sample, 10% of expired Lending Club loans, grades A-D\n",
        "# Source: https://www.lendingclub.com/info/download-data.action\n",
        "history = pd.read_csv(history_location)\n",
        "history['issue_d'] = pd.to_datetime(history['issue_d'], infer_datetime_format=True)\n",
        "\n",
        "# Current loans available for manual investing, June 17, 2019\n",
        "# Source: https://www.lendingclub.com/browse/browse.action\n",
        "current = pd.read_csv(current_location)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DqBdyqK5R2M2"
      },
      "source": [
        "### Wrangle data\n",
        "- Engineer date-based features\n",
        "- Remove features to avoid leakage\n",
        "- Do 3-way split, train/validate/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tQ9J0BH6R2M2",
        "colab": {}
      },
      "source": [
        "# Engineer date-based features\n",
        "\n",
        "# Transform earliest_cr_line to an integer:\n",
        "# How many days the earliest credit line was open, before the loan was issued.\n",
        "# For current loans available for manual investing, assume the loan will be issued today.\n",
        "history['earliest_cr_line'] = pd.to_datetime(history['earliest_cr_line'], infer_datetime_format=True)\n",
        "history['earliest_cr_line'] = history['issue_d'] - history['earliest_cr_line']\n",
        "history['earliest_cr_line'] = history['earliest_cr_line'].dt.days\n",
        "\n",
        "current['earliest_cr_line'] = pd.to_datetime(current['earliest_cr_line'], infer_datetime_format=True)\n",
        "current['earliest_cr_line'] = pd.Timestamp.today() - current['earliest_cr_line']\n",
        "current['earliest_cr_line'] = current['earliest_cr_line'].dt.days\n",
        "\n",
        "# Transform earliest_cr_line for the secondary applicant\n",
        "history['sec_app_earliest_cr_line'] = pd.to_datetime(history['sec_app_earliest_cr_line'], infer_datetime_format=True, errors='coerce')\n",
        "history['sec_app_earliest_cr_line'] = history['issue_d'] - history['sec_app_earliest_cr_line']\n",
        "history['sec_app_earliest_cr_line'] = history['sec_app_earliest_cr_line'].dt.days\n",
        "\n",
        "current['sec_app_earliest_cr_line'] = pd.to_datetime(current['sec_app_earliest_cr_line'], infer_datetime_format=True, errors='coerce')\n",
        "current['sec_app_earliest_cr_line'] = pd.Timestamp.today() - current['sec_app_earliest_cr_line']\n",
        "current['sec_app_earliest_cr_line'] = current['sec_app_earliest_cr_line'].dt.days\n",
        "\n",
        "# Engineer features for issue date year & month\n",
        "history['issue_d_year'] = history['issue_d'].dt.year\n",
        "history['issue_d_month'] = history['issue_d'].dt.month\n",
        "\n",
        "current['issue_d_year'] = pd.Timestamp.today().year\n",
        "current['issue_d_month'] = pd.Timestamp.today().month"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6IWaeCImaAD",
        "colab_type": "text"
      },
      "source": [
        "### Use a subset of features\n",
        "\n",
        "What subset of features should we use, to avoid leakage?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hbgClUDCR2M4",
        "colab": {}
      },
      "source": [
        "# Use Python sets to compare the historical columns & current columns\n",
        "\n",
        "common_columns = set(history.columns) & set(current.columns)\n",
        "just_history = set(history.columns) - set(current.columns)\n",
        "just_current = set(current.columns) - set(history.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s9e_llYWR2M6",
        "colab": {}
      },
      "source": [
        "# Train on the historical data.\n",
        "# For features, use only the common columns shared by the historical & current data.\n",
        "# For the target, use `loan_status` ('Fully Paid' or 'Charged Off')\n",
        "\n",
        "features = list(common_columns)\n",
        "target = 'loan_status'\n",
        "X = history[features]\n",
        "y = history[target]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iOoxx1wWR2M8",
        "colab": {}
      },
      "source": [
        "# Do train/validate/test 3-way split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y, test_size=20000, stratify=y, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval, test_size=20000, \n",
        "    stratify=y_trainval, random_state=42)\n",
        "\n",
        "print('X_train shape', X_train.shape)\n",
        "print('y_train shape', y_train.shape)\n",
        "print('X_val shape', X_val.shape)\n",
        "print('y_val shape', y_val.shape)\n",
        "print('X_test shape', X_test.shape)\n",
        "print('y_test shape', y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aFLirxfHR2M-"
      },
      "source": [
        "## Understand why accuracy is a misleading metric when classes are imbalanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0-KxM9aJR2M-"
      },
      "source": [
        "### Get accuracy score for majority class baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LWOZz76-Vt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.float_format = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZRu7VrJpR2M_",
        "colab": {}
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ScnJsYSDeEu8",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "majority_class = y_train.mode()[0]\n",
        "y_pred = np.full_like(y_val, fill_value=majority_class)\n",
        "accuracy_score(y_val, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KHFJ4dBuR2NA"
      },
      "source": [
        "### Get confusion matrix for majority class baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HbQaHeabR2NA",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    labels = unique_labels(y_true)\n",
        "    columns = [f'Predicted {label}' for label in labels]\n",
        "    index = [f'Actual {label}' for label in labels]\n",
        "    table = pd.DataFrame(confusion_matrix(y_true, y_pred), \n",
        "                         columns=columns, index=index)\n",
        "    return sns.heatmap(table, annot=True, fmt='d', cmap='viridis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "snBr6NkLR2NC",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(y_val, y_pred);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "afQOq2ndR2ND"
      },
      "source": [
        "### Get precision & recall for majority class baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hrf7NHWbR2NE",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wA9UIWr0R2NF"
      },
      "source": [
        "### Get ROC AUC score for majority class baseline\n",
        "[sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xOe_uTZcR2NF",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# What if we predicted 100% probability of the positive class for every prediction?\n",
        "# This is like the majority class baseline, but with predicted probabilities,\n",
        "# instead of just discrete classes.\n",
        "# VERY IMPORTANT ‚Äî¬†Use predicted probabilities with ROC AUC score!\n",
        "# Because, it's a metric of how well you rank/sort predicted probabilities.\n",
        "y_pred_proba = np.full_like(y_val, fill_value=1.00)\n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vydmoSBchPS7",
        "colab": {}
      },
      "source": [
        "# ROC AUC is 0.50 by definition when predicting any constant probability value\n",
        "y_pred_proba = np.full_like(y_val, fill_value=0)\n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hjK8o9ZChkeV",
        "colab": {}
      },
      "source": [
        "y_pred_proba = np.full_like(y_val, fill_value=0.50)\n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peDsUqEYyFuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_val.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50tnSds0yFuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot ROC curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_val=='Charged Off', y_pred_proba)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4DiIg0-_R2NH"
      },
      "source": [
        "### Fit a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w1USCwFnR2NH"
      },
      "source": [
        "#### Count missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-mrGaDCR2NI",
        "colab": {}
      },
      "source": [
        "null_counts = X_train.isnull().sum().sort_values(ascending=False)\n",
        "null_counts.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XxgDKuQuR2NJ",
        "colab": {}
      },
      "source": [
        "many_nulls = null_counts[:73].index\n",
        "print(list(many_nulls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7puVifaPR2NM"
      },
      "source": [
        "#### Wrangle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hO2F3AdtR2NM",
        "colab": {}
      },
      "source": [
        "def wrangle(X):\n",
        "    X = X.copy()\n",
        "\n",
        "    # Engineer new feature for every feature: is the feature null?\n",
        "    for col in X:\n",
        "        X[col+'_NULL'] = X[col].isnull()\n",
        "    \n",
        "    # Convert percentages from strings to floats\n",
        "    X['int_rate'] = X['int_rate'].str.strip('%').astype(float)\n",
        "    X['revol_util'] = X['revol_util'].str.strip('%').astype(float)\n",
        "    \n",
        "    # Convert employment length from string to float\n",
        "    X['emp_length'] = X['emp_length'].str.replace(r'\\D','').astype(float)\n",
        "        \n",
        "    # Create features for three employee titles: teacher, manager, owner\n",
        "    X['emp_title'] = X['emp_title'].str.lower()\n",
        "    X['emp_title_teacher'] = X['emp_title'].str.contains('teacher', na=False)\n",
        "    X['emp_title_manager'] = X['emp_title'].str.contains('manager', na=False)\n",
        "    X['emp_title_owner']   = X['emp_title'].str.contains('owner', na=False)\n",
        "\n",
        "    # Get length of free text fields\n",
        "    X['title'] = X['title'].str.len()\n",
        "    X['desc'] = X['desc'].str.len()\n",
        "    X['emp_title'] = X['emp_title'].str.len()\n",
        "    \n",
        "    # Convert sub_grade from string \"A1\"-\"D5\" to integer 1-20\n",
        "    sub_grade_ranks = {'A1': 1, 'A2': 2, 'A3': 3, 'A4': 4, 'A5': 5, 'B1': 6, 'B2': 7, \n",
        "                       'B3': 8, 'B4': 9, 'B5': 10, 'C1': 11, 'C2': 12, 'C3': 13, 'C4': 14, \n",
        "                       'C5': 15, 'D1': 16, 'D2': 17, 'D3': 18, 'D4': 19, 'D5': 20}\n",
        "    X['sub_grade'] = X['sub_grade'].map(sub_grade_ranks)\n",
        "    \n",
        "    # Drop some columns\n",
        "    X = X.drop(columns='id')        # Always unique\n",
        "    X = X.drop(columns='url')       # Always unique\n",
        "    X = X.drop(columns='member_id') # Always null\n",
        "    X = X.drop(columns='grade')     # Duplicative of sub_grade\n",
        "    X = X.drop(columns='zip_code')  # High cardinality\n",
        "    \n",
        "    # Only use these features which had nonzero permutation importances in earlier models    \n",
        "    features = ['acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc', \n",
        "                'annual_inc_joint', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', \n",
        "                'collections_12_mths_ex_med', 'delinq_amnt', 'desc_NULL', 'dti', \n",
        "                'dti_joint', 'earliest_cr_line', 'emp_length', 'emp_length_NULL', \n",
        "                'emp_title', 'emp_title_NULL', 'emp_title_owner', 'fico_range_high', \n",
        "                'funded_amnt', 'home_ownership', 'inq_last_12m', 'inq_last_6mths', \n",
        "                'installment', 'int_rate', 'issue_d_month', 'issue_d_year', 'loan_amnt', \n",
        "                'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', \n",
        "                'mo_sin_rcnt_rev_tl_op', 'mort_acc', 'mths_since_last_major_derog_NULL', \n",
        "                'mths_since_last_record', 'mths_since_recent_bc', 'mths_since_recent_inq', \n",
        "                'num_actv_bc_tl', 'num_actv_rev_tl', 'num_op_rev_tl', 'num_rev_tl_bal_gt_0', \n",
        "                'num_tl_120dpd_2m_NULL', 'open_rv_12m_NULL', 'open_rv_24m', \n",
        "                'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'purpose', \n",
        "                'revol_bal', 'revol_bal_joint', 'sec_app_earliest_cr_line', \n",
        "                'sec_app_fico_range_high', 'sec_app_open_acc', 'sec_app_open_act_il', \n",
        "                'sub_grade', 'term', 'title', 'title_NULL', 'tot_coll_amt', \n",
        "                'tot_hi_cred_lim', 'total_acc', 'total_bal_il', 'total_bc_limit', \n",
        "                'total_cu_tl', 'total_rev_hi_lim']    \n",
        "    X = X[features]\n",
        "    \n",
        "    # Return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "\n",
        "X_train = wrangle(X_train)\n",
        "X_val   = wrangle(X_val)\n",
        "X_test  = wrangle(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlFxLAnrR2NO",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "import category_encoders as ce\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='median'), \n",
        "    RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ihp_kbdGR2NQ"
      },
      "source": [
        "### Get accuracy score for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDqQXwPCR2NQ",
        "colab": {}
      },
      "source": [
        "y_pred = pipeline.predict(X_val)\n",
        "accuracy_score(y_val, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-GootfQvR2NU"
      },
      "source": [
        "### Get confusion matrix for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vCDL4JA1R2NU",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(y_val, y_pred);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FaaOcOxQR2NW"
      },
      "source": [
        "### Get precision & recall for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JiNdsscoR2NX",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VbTRfBJRR2NZ"
      },
      "source": [
        "### Get ROC AUC score for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OOWwRVGOR2Na",
        "colab": {}
      },
      "source": [
        "y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
        "roc_auc_score(y_val, y_pred_proba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vPv8SuZMR2Nb"
      },
      "source": [
        "## Understand ROC AUC (Receiver Operating Characteristic, Area Under the Curve)\n",
        "\n",
        "#### Scikit-Learn docs\n",
        "- [User Guide: Receiver operating characteristic (ROC)](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc)\n",
        "- [sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
        "- [sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
        "\n",
        "#### More links\n",
        "- [ROC curves and Area Under the Curve explained](https://www.dataschool.io/roc-curves-and-auc-explained/)\n",
        "- [The philosophical argument for using ROC curves](https://lukeoakdenrayner.wordpress.com/2018/01/07/the-philosophical-argument-for-using-roc-curves/)\n",
        "\n",
        "[Wikipedia explains,](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) \"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\"\n",
        "\n",
        "ROC AUC is the area under the ROC curve. [It can be interpreted](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it) as \"the expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative.\" \n",
        "\n",
        "ROC AUC measures how well a classifier ranks predicted probabilities. It ranges from 0 to 1. A naive majority class baseline will have an ROC AUC score of 0.5. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nBhPT1FHR2Nb"
      },
      "source": [
        "## Visualize the ROC curve by plotting true positive rate vs false positive rate at varying thresholds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ytNvg-wsR2Nc",
        "colab": {}
      },
      "source": [
        "from ipywidgets import interact, fixed\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def set_threshold(y_true, y_pred_proba, threshold=0.5):\n",
        "    \"\"\"\n",
        "    For binary classification problems. \n",
        "    y_pred_proba : predicted probability of class 1\n",
        "    \"\"\"\n",
        "    \n",
        "    # Apply threshold to predicted probabilities\n",
        "    # to get discrete predictions\n",
        "    class_0, class_1 = unique_labels(y_true)\n",
        "    y_pred = np.full_like(y_true, fill_value=class_0)\n",
        "    y_pred[y_pred_proba > threshold] = class_1\n",
        "    \n",
        "    # Plot distribution of predicted probabilities\n",
        "    ax = sns.distplot(y_pred_proba)\n",
        "    ax.axvline(threshold, color='red')\n",
        "    plt.title('Distribution of predicted probabilities')\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate true positive rate and false positive rate\n",
        "    true_positives = (y_pred==y_true) & (y_pred==class_1)\n",
        "    false_positives = (y_pred!=y_true) & (y_pred==class_1)\n",
        "    actual_positives = (y_true==class_1)\n",
        "    actual_negatives = (y_true==class_0)\n",
        "    true_positive_rate = true_positives.sum() / actual_positives.sum()\n",
        "    false_positive_rate = false_positives.sum() / actual_negatives.sum()\n",
        "    print('False Positive Rate', false_positive_rate)\n",
        "    print('True Positive Rate', true_positive_rate)\n",
        "    \n",
        "    # Plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_true==class_1, y_pred_proba)\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.title('ROC curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    \n",
        "    # Plot point on ROC curve for the current threshold\n",
        "    plt.scatter(false_positive_rate, true_positive_rate)\n",
        "    plt.show()\n",
        "    \n",
        "    # Show ROC AUC score\n",
        "    print('Area under the Receiver Operating Characteristic curve:', \n",
        "          roc_auc_score(y_true, y_pred_proba))\n",
        "    \n",
        "    # Show confusion matrix & classification report\n",
        "    plot_confusion_matrix(y_true, y_pred)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "interact(set_threshold, \n",
        "         y_true=fixed(y_val), \n",
        "         y_pred_proba=fixed(y_pred_proba), \n",
        "         threshold=(0,1,0.05));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DWBVRiZdR2Nd"
      },
      "source": [
        "### BONUS: Use the class_weight parameter in scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XTcCfxvgR2Nd"
      },
      "source": [
        "Here's a fun demo you can explore! The next code cells do five things:\n",
        "\n",
        "#### 1. Generate data\n",
        "\n",
        "We use scikit-learn's [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function to generate fake data for a binary classification problem, based on several parameters, including:\n",
        "- Number of samples\n",
        "- Weights, meaning \"the proportions of samples assigned to each class.\"\n",
        "- Class separation: \"Larger values spread out the clusters/classes and make the classification task easier.\"\n",
        "\n",
        "(We are generating fake data so it is easy to visualize.)\n",
        "\n",
        "#### 2. Split data\n",
        "\n",
        "We split the data three ways, into train, validation, and test sets. (For this toy example, it's not really necessary to do a three-way split. A two-way split, or even no split, would be ok. But I'm trying to demonstrate good habits, even in toy examples, to avoid confusion.)\n",
        "\n",
        "#### 3. Fit model\n",
        "\n",
        "We use scikit-learn to fit a [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on the training data.\n",
        "\n",
        "We use this model parameter:\n",
        "\n",
        "> **class_weight : _dict or ‚Äòbalanced‚Äô, default: None_**\n",
        "\n",
        "> Weights associated with classes in the form `{class_label: weight}`. If not given, all classes are supposed to have weight one.\n",
        "\n",
        "> The ‚Äúbalanced‚Äù mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`.\n",
        "\n",
        "\n",
        "#### 4. Evaluate model\n",
        "\n",
        "We use our Logistic Regression model, which was fit on the training data, to generate predictions for the validation data.\n",
        "\n",
        "Then we print [scikit-learn's Classification Report](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report), with many metrics, and also the accuracy score. We are comparing the correct labels to the Logistic Regression's predicted labels, for the validation set. \n",
        "\n",
        "#### 5. Visualize decision function\n",
        "\n",
        "Based on these examples\n",
        "- https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/combine/plot_comparison_combine.html\n",
        "- http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/#example-1-decision-regions-in-2d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8G277Q2CR2Ne",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_validation_test_split(\n",
        "    X, y, train_size=0.8, val_size=0.1, test_size=0.1, \n",
        "    random_state=None, shuffle=True):\n",
        "        \n",
        "    assert train_size + val_size + test_size == 1\n",
        "    \n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, shuffle=shuffle)\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=val_size/(train_size+val_size), \n",
        "        random_state=random_state, shuffle=shuffle)\n",
        "    \n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a3NHQGQ_R2Ng",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "\n",
        "#1. Generate data\n",
        "\n",
        "# Try re-running the cell with different values for these parameters\n",
        "n_samples = 1000\n",
        "weights = (0.95, 0.05)\n",
        "class_sep = 0.8\n",
        "\n",
        "X, y = make_classification(n_samples=n_samples, n_features=2, n_informative=2, \n",
        "                           n_redundant=0, n_repeated=0, n_classes=2, \n",
        "                           n_clusters_per_class=1, weights=weights, \n",
        "                           class_sep=class_sep, random_state=0)\n",
        "\n",
        "\n",
        "# 2. Split data\n",
        "\n",
        "# Uses our custom train_validation_test_split function\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n",
        "    X, y, train_size=0.8, val_size=0.1, test_size=0.1, random_state=1)\n",
        "\n",
        "\n",
        "# 3. Fit model\n",
        "\n",
        "# Try re-running the cell with different values for this parameter\n",
        "class_weight = {0: 1, 1: 10000}\n",
        "\n",
        "model = LogisticRegression(solver='lbfgs', class_weight=class_weight)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# 4. Evaluate model\n",
        "\n",
        "y_pred = model.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))\n",
        "plot_confusion_matrix(y_val, y_pred)\n",
        "\n",
        "# 5. Visualize decision regions\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_decision_regions(X_val, y_val, model, legend=0);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P3rGNwAjR2Nh"
      },
      "source": [
        "Try re-running the cell above with different values for these four parameters:\n",
        "- `n_samples`\n",
        "- `weights`\n",
        "- `class_sep`\n",
        "- `class_balance`\n",
        "\n",
        "For example, with a 50% / 50% class distribution:\n",
        "```\n",
        "n_samples = 1000\n",
        "weights = (0.50, 0.50)\n",
        "class_sep = 0.8\n",
        "class_balance = None\n",
        "```\n",
        "\n",
        "With a 95% / 5% class distribution:\n",
        "```\n",
        "n_samples = 1000\n",
        "weights = (0.95, 0.05)\n",
        "class_sep = 0.8\n",
        "class_balance = None\n",
        "```\n",
        "\n",
        "With the same 95% / 5% class distribution, but changing the Logistic Regression's `class_balance` parameter to `'balanced'` (instead of its default `None`)\n",
        "```\n",
        "n_samples = 1000\n",
        "weights = (0.95, 0.05)\n",
        "class_sep = 0.8\n",
        "class_balance = 'balanced'\n",
        "```\n",
        "\n",
        "With the same 95% / 5% class distribution, but with different values for `class_balance`:\n",
        "- `{0: 1, 1: 1}` _(equivalent to `None`)_\n",
        "- `{0: 1, 1: 2}`\n",
        "- `{0: 1, 1: 10}` _(roughly equivalent to `'balanced'` for this dataset)_\n",
        "- `{0: 1, 1: 100}`\n",
        "- `{0: 1, 1: 10000}`\n",
        "\n",
        "How do the evaluation metrics and decision region plots change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9GhisE6vR2Ni"
      },
      "source": [
        "## What you can do about imbalanced classes\n",
        "\n",
        "[Learning from Imbalanced Classes](https://www.svds.com/tbt-learning-imbalanced-classes/) gives \"a rough outline of useful approaches\" : \n",
        "\n",
        "- Do nothing. Sometimes you get lucky and nothing needs to be done. You can train on the so-called natural (or stratified) distribution and sometimes it works without need for modification.\n",
        "- Balance the training set in some way:\n",
        "  - Oversample the minority class.\n",
        "  - Undersample the majority class.\n",
        "  - Synthesize new minority classes.\n",
        "- Throw away minority examples and switch to an anomaly detection framework.\n",
        "- At the algorithm level, or after it:\n",
        "  - Adjust the class weight (misclassification costs).\n",
        "  - Adjust the decision threshold.\n",
        "  - Modify an existing algorithm to be more sensitive to rare classes.\n",
        "- Construct an entirely new algorithm to perform well on imbalanced data.\n",
        "\n",
        "#### We demonstrated two of these options: \n",
        "\n",
        "- \"Adjust the class weight (misclassification costs)\" ‚Äî many scikit-learn classifiers have a `class_balance` parameter\n",
        "- \"Adjust the decision threshold\" ‚Äî you can lean more about this in a great blog post, [Visualizing Machine Learning Thresholds to Make Better Business Decisions](https://blog.insightdatascience.com/visualizing-machine-learning-thresholds-to-make-better-business-decisions-4ab07f823415).\n",
        "\n",
        "#### Another option to be aware of:\n",
        "- The [imbalance-learn](https://github.com/scikit-learn-contrib/imbalanced-learn) library can be used to \"oversample the minority class, undersample the majority class, or synthesize new minority classes.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kxq8HAEgcc_",
        "colab_type": "text"
      },
      "source": [
        "-----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkB4a8U-geI_",
        "colab_type": "text"
      },
      "source": [
        "---------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnpPLCCegfn-",
        "colab_type": "text"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "**You will use your portfolio project dataset for all assignments this sprint.**\n",
        "\n",
        "Complete these tasks for your project, and document your decisions.\n",
        "\n",
        "- [ ] Choose your target. Which column in your tabular dataset will you predict?\n",
        "- [ ] Choose which observations you will use to train, validate, and test your model. And which observations, if any, to exclude.\n",
        "- [ ] Determine whether your problem is regression or classification.\n",
        "- [ ] Choose your evaluation metric.\n",
        "- [ ] Begin with baselines: majority class baseline for classification, or mean baseline for regression, with your metric of choice.\n",
        "- [ ] Begin to clean and explore your data.\n",
        "- [ ] Begin to choose which features, if any, to exclude. Would some features \"leak\" information from the future?\n",
        "\n",
        "## Reading\n",
        "\n",
        "### ROC AUC\n",
        "- [Machine Learning Meets Economics](http://blog.mldb.ai/blog/posts/2016/01/ml-meets-economics/)\n",
        "- [ROC curves and Area Under the Curve explained](https://www.dataschool.io/roc-curves-and-auc-explained/)\n",
        "- [The philosophical argument for using ROC curves](https://lukeoakdenrayner.wordpress.com/2018/01/07/the-philosophical-argument-for-using-roc-curves/)\n",
        "\n",
        "### Imbalanced Classes\n",
        "- [imbalance-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)\n",
        "- [Learning from Imbalanced Classes](https://www.svds.com/tbt-learning-imbalanced-classes/)\n",
        "\n",
        "### Last lesson\n",
        "- [Attacking discrimination with smarter machine learning](https://research.google.com/bigpicture/attacking-discrimination-in-ml/), by Google Research, with  interactive visualizations. _\"A threshold classifier essentially makes a yes/no decision, putting things in one category or another. We look at how these classifiers work, ways they can potentially be unfair, and how you might turn an unfair classifier into a fairer one. As an illustrative example, we focus on loan granting scenarios where a bank may grant or deny a loan based on a single, automatically computed number such as a credit score.\"_\n",
        "- [How Shopify Capital Uses Quantile Regression To Help Merchants Succeed](https://engineering.shopify.com/blogs/engineering/how-shopify-uses-machine-learning-to-help-our-merchants-grow-their-business)\n",
        "- [Maximizing Scarce Maintenance Resources with Data: Applying predictive modeling, precision at k, and clustering to optimize impact](https://towardsdatascience.com/maximizing-scarce-maintenance-resources-with-data-8f3491133050), by Lambda DS3 student Michael Brady. His blog post extends the Tanzania Waterpumps scenario, far beyond what's in the lecture notebook.\n",
        "- [Notebook about how to calculate expected value from a confusion matrix by treating it as a cost-benefit matrix](https://github.com/podopie/DAT18NYC/blob/master/classes/13-expected_value_cost_benefit_analysis.ipynb)\n",
        "- [Simple guide to confusion matrix terminology](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) by Kevin Markham, with video\n",
        "- [Visualizing Machine Learning Thresholds to Make Better Business Decisions](https://blog.insightdatascience.com/visualizing-machine-learning-thresholds-to-make-better-business-decisions-4ab07f823415)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBvFKwk0gdkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}